{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Overview","text":"<p>A Python framework for autonomous agent networks that handle task automation with multi-step reasoning.</p> <p>Visit:</p> <ul> <li>Playground</li> <li>Documentation</li> <li>Github</li> <li>Python SDK</li> </ul>"},{"location":"index.html#key-features","title":"Key Features","text":"<p><code>versionhq</code> is a Python framework designed for automating complex, multi-step tasks using autonomous agent networks.</p> <p>Users can either configure their agents and network manually or allow the system to automatically manage the process based on provided task goals.</p>"},{"location":"index.html#agent-network","title":"Agent Network","text":"<p>Agents adapt their formation based on task complexity.</p> <p>You can specify a desired formation or allow the agents to determine it autonomously (default).</p> Solo Agent Supervising Squad Random Formation Usage <ul><li>A single agent with tools, knowledge, and memory.</li><li>When self-learning mode is on - it will turn into Random formation.</li></ul> <ul><li>Leader agent gives directions, while sharing its knowledge and memory.</li><li>Subordinates can be solo agents or networks.</li></ul> <ul><li>Share tasks, knowledge, and memory among network members.</li></ul> <ul><li>A single agent handles tasks, asking help from other agents without sharing its memory or knowledge.</li></ul> Use case An email agent drafts promo message for the given audience. The leader agent strategizes an outbound campaign plan and assigns components such as media mix or message creation to subordinate agents. An email agent and social media agent share the product knowledge and deploy multi-channel outbound campaign. 1. An email agent drafts promo message for the given audience, asking insights on tones from other email agents which oversee other clusters. 2. An agent calls the external agent to deploy the campaign."},{"location":"index.html#graph-theory-concept","title":"Graph Theory Concept","text":"<p>To completely automate task workflows, agents will build a <code>task-oriented network</code> by generating <code>nodes</code> that represent tasks and connecting them with dependency-defining <code>edges</code>.</p> <p>Each node is triggered by specific events and executed by an assigned agent once all dependencies are met.</p> <p>While the network automatically reconfigures itself, you retain the ability to direct the agents using <code>should_reform</code> variable.</p> <p>The following code snippet explicitly demonstrates the <code>TaskGraph</code> and its visualization, saving the diagram to the <code>uploads</code> directory.</p> <pre><code>import versionhq as vhq\n\ntask_graph = vhq.TaskGraph(directed=False, should_reform=True) # triggering auto formation\n\ntask_a = vhq.Task(description=\"Research Topic\")\ntask_b = vhq.Task(description=\"Outline Post\")\ntask_c = vhq.Task(description=\"Write First Draft\")\n\nnode_a = task_graph.add_task(task=task_a)\nnode_b = task_graph.add_task(task=task_b)\nnode_c = task_graph.add_task(task=task_c)\n\ntask_graph.add_dependency(\n   node_a.identifier, node_b.identifier,\n   dependency_type=vhq.DependencyType.FINISH_TO_START, weight=5, description=\"B depends on A\"\n)\ntask_graph.add_dependency(\n   node_a.identifier, node_c.identifier,\n   dependency_type=vhq.DependencyType.FINISH_TO_FINISH, lag=1, required=False, weight=3\n)\n\n# To visualize the graph:\ntask_graph.visualize()\n\n# To start executing nodes:\nlatest_output, outputs = task_graph.activate()\n\nassert isinstance(last_task_output, vhq.TaskOutput)\nassert [k in task_graph.nodes.keys() and v and isinstance(v, vhq.TaskOutput) for k, v in outputs.items()]\n</code></pre>"},{"location":"index.html#task-graph","title":"Task Graph","text":"<p>A <code>TaskGraph</code> represents tasks as <code>nodes</code> and their execution dependencies as <code>edges</code>, automating rule-based execution.</p> <p><code>Agent Networks</code> can handle <code>TaskGraph</code> objects by optimizing their formations.</p> <p></p> <ul> <li>Ref: TaskGraph class</li> </ul>"},{"location":"index.html#optimization","title":"Optimization","text":"<p>Autonomous agents are model-agnostic and can leverage their own and their peers' knowledge sources, memories, and tools.</p> <p>Agents are optimized during network formation, but customization is possible before or after.</p> <p>The following code snippet demonstrates agent customization:</p> <pre><code>import versionhq as vhq\n\nagent = vhq.Agent(role=\"Marketing Analyst\")\n\n# update the agent\nagent.update(\n   llm=\"gemini-2.0\", # updating LLM (Valid llm_config will be inherited to the new LLM.)\n   tools=[vhq.Tool(func=lambda x: x)], # adding tools\n   max_rpm=3,\n   knowledge_sources=[\"&lt;KC1&gt;\", \"&lt;KS2&gt;\"], # adding knowledge sources. This will trigger the storage creation.\n   memory_config={\"user_id\": \"0001\"}, # adding memories\n   dummy=\"I am dummy\" # &lt;- invalid field will be automatically ignored\n)\n</code></pre>"},{"location":"index.html#project-set-up","title":"Project Set Up","text":"<p>Installing package manager</p> <p>For MacOS:</p> <pre><code>brew install uv\n</code></pre> <p>For Ubuntu/Debian:    <pre><code>sudo apt-get install uv\n</code></pre></p> <p>Installing dependencies</p> <pre><code>uv venv\nsource .venv/bin/activate\nuv lock --upgrade\nuv sync --all-extras\n</code></pre> <ul> <li> <p>AssertionError/module mismatch errors: Set up default Python version using <code>.pyenv</code> <pre><code>pyenv install 3.12.8\npyenv global 3.12.8  (optional: `pyenv global system` to get back to the system default ver.)\nuv python pin 3.12.8\necho 3.12.8 &gt;&gt; .python-version\n</code></pre></p> </li> <li> <p><code>pygraphviz</code> related errors: Run the following commands:       <pre><code>brew install graphviz\nuv pip install --config-settings=\"--global-option=build_ext\" \\\n--config-settings=\"--global-option=-I$(brew --prefix graphviz)/include/\" \\\n--config-settings=\"--global-option=-L$(brew --prefix graphviz)/lib/\" \\\npygraphviz\n</code></pre></p> <ul> <li>If the error continues, skip pygraphviz installation by:</li> </ul> <pre><code>uv sync --all-extras --no-extra pygraphviz\n</code></pre> </li> </ul> <p>Setting up a local env file</p> <p>Create <code>.env</code> file at the root of the project directry and add your keys following <code>.env.sample</code>.</p>"},{"location":"index.html#technologies-used","title":"Technologies Used","text":"<p>Schema, Data Validation</p> <ul> <li> <p>Pydantic: Data validation and serialization library for Python.</p> </li> <li> <p>Upstage: Document processer for ML tasks. (Use <code>Document Parser API</code> to extract data from documents)</p> </li> <li> <p>Docling: Document parsing</p> </li> </ul> <p>Workflow, Task Graph</p> <ul> <li> <p>NetworkX: A Python package to analyze, create, and manipulate complex graph networks.</p> </li> <li> <p>Matplotlib: For graph visualization.</p> </li> <li> <p>Graphviz: For graph visualization.</p> </li> </ul> <p>LLM Curation</p> <ul> <li>LiteLLM: LLM orchestration platform</li> </ul> <p>Tools</p> <ul> <li>Composio: Conect RAG agents with external tools, Apps, and APIs to perform actions and receive triggers. We use tools and RAG tools from Composio toolset.</li> </ul> <p>Storage</p> <ul> <li> <p>mem0ai: Agents' memory storage and management.</p> </li> <li> <p>Chroma DB: Vector database for storing and querying usage data.</p> </li> <li> <p>SQLite: C-language library to implements a small SQL database engine.</p> </li> </ul> <p>Deployment</p> <ul> <li> <p>uv: Python package installer and resolver</p> </li> <li> <p>pre-commit: Manage and maintain pre-commit hooks</p> </li> <li> <p>setuptools: Build python modules</p> </li> </ul>"},{"location":"index.html#trouble-shooting","title":"Trouble Shooting","text":"<p>Common issues and solutions:</p> <ul> <li> <p>API key errors: Ensure all API keys in the <code>.env</code> file are correct and up to date. Make sure to add <code>load_dotenv()</code> on the top of the python file to apply the latest environment values.</p> </li> <li> <p>Database connection issues: Check if the Chroma DB is properly initialized and accessible.</p> </li> <li> <p>Memory errors: If processing large contracts, you may need to increase the available memory for the Python process.</p> </li> <li> <p>Issues related to dependencies: <code>rm -rf uv.lock</code>, <code>uv cache clean</code>, <code>uv venv</code>, and run <code>uv pip install -r requirements.txt -v</code>.</p> </li> <li> <p>Issues related to <code>torch</code> installation: Add optional dependencies by <code>uv add versionhq[torch]</code>.</p> </li> <li> <p>Issues related to agents and other systems: Check <code>.logs</code> directory located at the root of the project directory for error messages and stack traces.</p> </li> <li> <p>Issues related to <code>Python quit unexpectedly</code>: Check this stackoverflow article.</p> </li> <li> <p><code>reportMissingImports</code> error from pyright after installing the package: This might occur when installing new libraries while VSCode is running. Open the command pallete (ctrl + shift + p) and run the Python: Restart language server task.</p> </li> </ul>"},{"location":"index.html#faq","title":"FAQ","text":"<p>Q. Where can I see if the agent is working?</p> <p>A. Visit playground.</p>"},{"location":"index.html#contributing","title":"Contributing","text":"<p><code>versionhq</code> is a open source project.</p> <p>Steps</p> <ol> <li> <p>Create your feature branch (<code>git checkout -b feature/your-amazing-feature</code>)</p> </li> <li> <p>Create amazing features</p> </li> <li> <p>Add a test funcition to the <code>tests</code> directory and run pytest.</p> </li> <li> <p>Add secret values defined in <code>.github/workflows/run_test.yml</code> to your Github <code>repository secrets</code> located at settings &gt; secrets &amp; variables &gt; Actions.</p> </li> <li>Run a following command:       <pre><code>uv run pytest tests -vv --cache-clear\n</code></pre></li> </ol> <p>Building a new pytest function</p> <ul> <li>Files added to the <code>tests</code> directory must end in <code>_test.py</code>.</li> <li> <p>Test functions within the files must begin with <code>test_</code>.</p> </li> <li> <p>Update <code>docs</code> accordingly.</p> </li> <li> <p>Pull the latest version of source code from the main branch (<code>git pull origin main</code>) *Address conflicts if any.</p> </li> <li> <p>Commit your changes (<code>git add .</code> / <code>git commit -m 'Add your-amazing-feature'</code>)</p> </li> <li> <p>Push to the branch (<code>git push origin feature/your-amazing-feature</code>)</p> </li> <li> <p>Open a pull request</p> </li> </ul> <p>Optional</p> <ul> <li> <p>Flag with <code>REFINEME</code> for any improvements needed and <code>FIXME</code> for any errors.</p> </li> <li> <p><code>Playground</code> is available at <code>https://versi0n.io</code>.</p> </li> </ul> <p>Package Management with UV</p> <ul> <li> <p>Add a package: <code>uv add &lt;package&gt;</code></p> </li> <li> <p>Remove a package: <code>uv remove &lt;package&gt;</code></p> </li> <li> <p>Run a command in the virtual environment: <code>uv run &lt;command&gt;</code></p> </li> <li> <p>After updating dependencies, update <code>requirements.txt</code> accordingly or run <code>uv pip freeze &gt; requirements.txt</code></p> </li> </ul> <p>Pre-commit Hooks</p> <ol> <li> <p>Install pre-commit hooks:    <pre><code>uv run pre-commit install\n</code></pre></p> </li> <li> <p>Run pre-commit checks manually:    <pre><code>uv run pre-commit run --all-files\n</code></pre></p> </li> </ol> <p>Pre-commit hooks help maintain code quality by running checks for formatting, linting, and other issues before each commit.</p> <ul> <li>To skip pre-commit hooks    <pre><code>git commit --no-verify -m \"your-commit-message\"\n</code></pre></li> </ul> <p>Documentation</p> <ul> <li> <p>To edit the documentation, see <code>docs</code> repository and edit the respective component.</p> </li> <li> <p>We use <code>mkdocs</code> to update the docs. You can run the doc locally at http://127.0.0.1:8000/:</p> </li> </ul> <pre><code>uv run python3 -m mkdocs serve --clean\n</code></pre> <ul> <li>To add a new page, update <code>mkdocs.yml</code> in the root. Refer to MkDocs documentation for more details.</li> </ul>"},{"location":"quickstart.html","title":"Quick Start","text":""},{"location":"quickstart.html#package-installation","title":"Package installation","text":"<pre><code>pip install versionhq\n</code></pre> <p>(Python 3.11 | 3.12 | 3.13)</p>"},{"location":"quickstart.html#forming-agent-networks","title":"Forming agent networks","text":"<p>You can generate a network of multiple agents depending on your task complexity.</p> <p>Here is a code snippet:</p> <pre><code>import versionhq as vhq\n\nnetwork = vhq.form_agent_network(\n   task=\"YOUR AMAZING TASK OVERVIEW\",\n   expected_outcome=\"YOUR OUTCOME EXPECTATION\",\n)\nres, _ = network.launch()\n</code></pre> <p>This will form a network with multiple agents on <code>Formation</code> and return results as a <code>TaskOutput</code> object, storing outputs in JSON, plane text, Pydantic model formats along with evaluation.</p>"},{"location":"quickstart.html#building-ai-agents","title":"Building AI agents","text":"<p>If you don't need to form a network or assign a specific agent to the network, you can simply build an agent using <code>Agent</code> model.</p> <p>Agents can execute tasks using <code>Task</code> model and return JSON format by default with plane text and pydantic model formats as options.</p> <pre><code>import versionhq as vhq\nfrom pydantic import BaseModel\n\nclass CustomOutput(BaseModel):\n   test1: str\n   test2: list[str]\n\ndef dummy_func(message: str, **kwargs) -&gt; str:\n   test1 = kwargs[\"test1\"] if kwargs and \"test1\" in kwargs else \"\"\n   test2 = kwargs[\"test2\"] if kwargs and \"test2\" in kwargs else \"\"\n   if test1 and test2:\n      return f\"\"\"{message}: {test1}, {\", \".join(test2)}\"\"\"\n\nagent = vhq.Agent(role=\"demo manager\")\n\ntask = vhq.Task(\n   description=\"Amazing task\",\n   response_schema=CustomOutput,\n   callback=dummy_func,\n   callback_kwargs=dict(message=\"Hi! Here is the result: \")\n)\n\nres = task.execute(agent=agent, context=\"amazing context to consider.\")\n\nassert isinstance(res, vhq.TaskOutput)\n</code></pre> <p>This will return a <code>TaskOutput</code> object that stores response in plane text, JSON, and Pydantic model: <code>CustomOutput</code> formats with a callback result, tool output (if given), and evaluation results (if given).</p> <pre><code>res == TaskOutput(\n   task_id=UUID('&lt;TASK UUID&gt;'),\n   raw='{\\\"test1\\\":\\\"random str\\\", \\\"test2\\\":[\\\"str item 1\\\", \\\"str item 2\\\", \\\"str item 3\\\"]}',\n   json_dict={'test1': 'random str', 'test2': ['str item 1', 'str item 2', 'str item 3']},\n   pydantic=&lt;class '__main__.CustomOutput'&gt;,\n   tool_output=None,\n   callback_output='Hi! Here is the result: random str, str item 1, str item 2, str item 3', # returned a plain text summary\n   evaluation=None\n)\n</code></pre>"},{"location":"quickstart.html#supervising","title":"Supervising","text":"<p>To create an agent network with one or more manager agents, designate members using the <code>is_manager</code> tag.</p> <pre><code>import versionhq as vhq\n\nagent_a = vhq.Agent(role=\"agent a\", goal=\"My amazing goals\", llm=\"llm-of-your-choice\")\nagent_b = vhq.Agent(role=\"agent b\", goal=\"My amazing goals\", llm=\"llm-of-your-choice\")\n\ntask_1 = vhq.Task(\n   description=\"Analyze the client's business model.\",\n   response_schema=[vhq.ResponseField(title=\"test1\", data_type=str, required=True),],\n   allow_delegation=True\n)\n\ntask_2 = vhq.Task(\n   description=\"Define a cohort.\",\n   response_schema=[vhq.ResponseField(title=\"test1\", data_type=int, required=True),],\n   allow_delegation=False\n)\n\nnetwork =vhq.AgentNetwork(\n   members=[\n      vhq.Member(agent=agent_a, is_manager=False, tasks=[task_1]),\n      vhq.Member(agent=agent_b, is_manager=True, tasks=[task_2]), # Agent B as a manager\n   ],\n)\nres, _ = network.launch()\n\nassert isinstance(res, vhq.NetworkOutput)\nassert \"agent b\" in task_1.processed_agents # agent_b delegated by agent_a\nassert \"agent b\" in task_2.processed_agents\n</code></pre> <p>This will return a list with dictionaries with keys defined in the <code>ResponseField</code> of each task.</p> <p>Tasks can be delegated to a manager, peers within the agent network, or a completely new agent.</p>"},{"location":"tags.html","title":"Tags","text":""},{"location":"tags.html#tag:agent-network","title":"Agent Network","text":"<ul> <li>            Agent          </li> <li>            Agent Network          </li> <li>            Configuring          </li> <li>            Generating          </li> <li>            LLM          </li> <li>            Reference          </li> </ul>"},{"location":"tags.html#tag:task-graph","title":"Task Graph","text":"<ul> <li>            Evaluating          </li> <li>            Outputs          </li> <li>            Task          </li> <li>            TaskGraph          </li> </ul>"},{"location":"tags.html#tag:utilities","title":"Utilities","text":"<ul> <li>            Knowledge          </li> <li>            Memory          </li> <li>            RAG Tool          </li> <li>            Tool          </li> </ul>"},{"location":"core/knowledge.html","title":"Knowledge","text":"<p><code>class</code> versionhq.knowledge.model.Knowledge <p>A Pydantic class to store <code>Knowledge</code> of the agent.</p>","tags":["Utilities"]},{"location":"core/memory.html","title":"Memory","text":"<p><code>class</code> versionhq.memory.model.Memory <p>A Pydantic class to store <code>Memory</code> of the agent.</p>","tags":["Utilities"]},{"location":"core/rag-tool.html","title":"RAG Tool","text":"<p><code>class</code> versionhq.tool.rag_tool.RagTool <p>A Pydantic class to store RAG tools that the agent will use when it executes the task.</p>","tags":["Utilities"]},{"location":"core/rag-tool.html#quick-start","title":"Quick Start","text":"<p>Similar to the <code>Tool</code> class, you can run the RAG tool using <code>url</code> and <code>query</code> variables.</p> <pre><code>import versionhq as vhq\n\nrt = vhq.RagTool(\n    url=\"https://github.com/chroma-core/chroma/issues/3233\",\n    query=\"What is the next action plan?\"\n)\nres = rt.run()\n\nassert rt.text is not None # text source from the url\nassert res is not None\n</code></pre>","tags":["Utilities"]},{"location":"core/rag-tool.html#using-with-agents","title":"Using with Agents","text":"<p>You can call a specific agent when you run a RAG tool.</p> <pre><code>import versionhq as vhq\n\nrt = vhq.RagTool(url=\"https://github.com/chroma-core/chroma/issues/3233\", query=\"What is the next action plan?\")\n\nagent = vhq.Agent(role=\"RAG Tool Tester\")\nres = rt.run(agent=agent)\n\nassert agent.knowledge_sources is not None\nassert rt.text is not None\nassert res is not None\n</code></pre> <p>Agents can own RAG tools.</p> <pre><code>import versionhq as vhq\n\nrt = vhq.RagTool(url=\"https://github.com/chroma-core/chroma/issues/3233\", query=\"What is the next action plan?\")\n\nagent = vhq.Agent(role=\"RAG Tool Tester\", tools=[rt]) # adding RAG tool/s\ntask = vhq.Task(description=\"return a simple response\", can_use_agent_tools=True, tool_res_as_final=True)\nres = task.execute(agent=agent)\n\nassert res.raw is not None\nassert res.tool_output is not None\n</code></pre>","tags":["Utilities"]},{"location":"core/rag-tool.html#variables","title":"Variables","text":"Variable Data Type Default Nullable Description <code>api_key_name</code> Optional[str] None True API key name in .env file. <code>api_endpoint</code> Optional[str] None True API endpoint. <code>url</code> Optional[str] None True URLs to extract the text source. <code>headers</code> Optional[Dict[str, Any]] dict() - Request headers <code>query</code> Optional[str] None True Query. <code>text</code> Optional[str] None True Text sources extracted from the URL or API call","tags":["Utilities"]},{"location":"core/rag-tool.html#class-methods","title":"Class Methods","text":"Method Params Returns Description <code>store_data</code> <p>agent: Optional[\"vhq.Agent\"] = Non</p> None Stores the retrieved data in the storage. <code>run</code> *args, **kwargs Any Execute the tool.","tags":["Utilities"]},{"location":"core/tool.html","title":"Tool","text":"<p><code>class</code> versionhq.tool.model.Tool <p>A Pydantic class to store the tool object.</p>","tags":["Utilities"]},{"location":"core/tool.html#quick-start","title":"Quick Start","text":"<p>By defining the function, you can let the agent start to use it when they get an approval.</p> <pre><code>import versionhq as vhq\n\ndef demo_func(message: str) -&gt; str:\n    return message + \"_demo\"\n\nmy_tool = vhq.Tool(func=demo_func)\nres = my_tool.run(params=dict(message=\"Hi!\"))\n\nassert res == \"Hi!_demo\"\n</code></pre> <p>e.g. Build an agent with a simple tool</p> <p>The tool result will be considered in the context when the agent call LLM.</p> <pre><code>import versionhq as vhq\n\ndef demo_func() -&gt; str:\n    return \"demo\"\n\nmy_tool = vhq.Tool(func=demo_func)\n\nagent = vhq.Agent(\n    role=\"Tool Handler\",\n    goal=\"efficiently use the given tools\",\n    tools=[my_tool, ]\n)\nassert agent.tools == [my_tool]\n</code></pre>","tags":["Utilities"]},{"location":"core/tool.html#toolset","title":"ToolSet","text":"<p><code>class</code> versionhq.tool.model.ToolSet <p>To add args to the tool and record the usage, use <code>toolset</code> instance.</p> <pre><code>from versionhq import Tool, ToolSet, Agent\n\ndef demo_func(message: str) -&gt; str:\n    return message + \"_demo\"\n\ntool_a = Tool(func=demo_func)\ntoolset = ToolSet(tool=tool_a, kwargs={\"message\": \"Hi\"})\n\nagent = Agent(\n    role=\"Tool Handler\",\n    goal=\"efficiently use the given tools\",\n    tools=[toolset,]\n)\nassert agent.tools == [toolset]\n</code></pre>","tags":["Utilities"]},{"location":"core/tool.html#customization","title":"Customization","text":"","tags":["Utilities"]},{"location":"core/tool.html#tool-name","title":"Tool name","text":"<p><code>[var]</code><code>name: Optional[str] = None</code></p> <p>By default, the tool name will be set as a function name, but you can also define a specific name for your tool.</p> <pre><code>from versionhq import Tool\n\ndef demo_func() -&gt; str:\n    return \"demo\"\n\nmy_tool = Tool(func=demo_func)\n\nassert my_tool.name == \"demo_func\"\n</code></pre> <pre><code>from versionhq import Tool\n\nmy_tool = Tool(name=\"my empty tool\", func=lambda x: x)\n\nassert my_tool.name == \"my empty tool\"\n</code></pre> <p>Tool names are used to call cached tools later.</p>","tags":["Utilities"]},{"location":"core/tool.html#tool-execution","title":"Tool Execution","text":"<p><code>[var]</code><code>[REQUIRED] func: [Callable | Any] = None</code></p> <p><code>[abstract class method]</code><code>_run(self, *args, **kwargs) -&gt; Any</code></p> <p>Simple function calling can be handled using an abstract class method <code>_run()</code>.</p> <p>Following is the simplest way to define a quick function and execute it with the method.</p> <pre><code>from versionhq import Tool\n\nmy_tool = Tool(func=lambda x: f\"demo-{x}\")\nres = my_tool._run(x=\"TESTING\")\n\nassert res == \"demo-TESTING\"\n</code></pre> <p>Another way to define the logic is to add a function to the <code>func</code> field.</p> <p>Functions are useful for defining more complex execution flows and reusing among multiple tools.</p> <pre><code>from versionhq import Tool\n\ndef demo_func() -&gt; str:\n    \"\"\"...some complex execution...\"\"\"\n    return \"demo\"\n\nmy_tool = Tool(func=demo_func)\nres = my_tool._run()\n\nassert res == \"demo\"\n</code></pre> <p>You can also pass parameters using the class method.</p> <pre><code>from versionhq import Tool\n\ndef demo_func(message: str) -&gt; str:\n    return message + \"_demo\"\n\nmy_tool = Tool(func=demo_func)\nres = my_tool._run(message=\"Hi!\")\n\nassert res == \"Hi!_demo\"\n</code></pre>","tags":["Utilities"]},{"location":"core/tool.html#custom-tool-execution","title":"Custom tool execution","text":"<p><code>[abstract class method]</code><code>_run**(self, *args, **kwargs) -&gt; Any</code></p> <p>You can also use the class method to execute your custom tool inherited from <code>Tool</code> instance.</p> <pre><code>from typing import Callable\nfrom versionhq import Tool\n\nclass MyCustomTool(Tool):\n  name: str = \"custom tool\"\n  func: Callable\n\nmy_custom_tool = MyCustomTool(func=lambda x: len(x))\nres = my_custom_tool._run([\"demo1\", \"demo2\"])\n\nassert res == 2\n</code></pre>","tags":["Utilities"]},{"location":"core/tool.html#cached-tool-execution","title":"Cached tool execution","text":"<p><code>[class_method]</code><code>run(self, params: Dict[str, Any]) -&gt; Any</code></p> <p>To use cached tools, call the class method <code>run</code> instead of _run.</p> <pre><code>from typing import List, Any, Callable\nfrom versionhq import Tool\n\nclass CustomTool(Tool):\n    name: str = \"custom tool\"\n    func: Callable\n\ndef demo_func(demo_list: List[Any]) -&gt; int:\n    return len(demo_list)\n\nmy_tool = CustomTool(func=demo_func)\nres = my_tool.run(params=dict(demo_list=[\"demo1\", \"demo2\"]))\n\nassert res == 2\nassert isinstance(my_tool.tool_handler, vhq.ToolHandler)\n</code></pre> <p>*Reference: <code>ToolHandler</code> class</p>","tags":["Utilities"]},{"location":"core/tool.html#cache","title":"Cache","text":"<p><code>[var]</code><code>cache_function: Callable[..., Any] = None</code></p> <p>Define a cache function to call.</p> <p><code>[var]</code><code>cache_handler: InstanceOf[CacheHandler] = None</code></p> <p>Define how to handle cache.</p> <p><code>[var]</code><code>should_cache: bool = True</code></p> <p>Define if the tool name and arguments should be cached or not.</p> <ul> <li>Reference: <code>Cache Handler</code> class</li> </ul>","tags":["Utilities"]},{"location":"core/tool.html#function-calling-llm","title":"Function Calling LLM","text":"<p>To use the tools with LLM, make sure the LLM supports function calling, then add the tool to the agent or task.</p>","tags":["Utilities"]},{"location":"core/tool.html#1-use-the-agents-tools","title":"1. Use the agent's tools","text":"<p>When the agent has tools, the tools will be applicable across multiple tasks that the agent will handle with approval of <code>can_use_agent_tools</code> on Task instance.</p> <p>i.e., Return the agent\u2019s tools result as a final result:</p> <pre><code>from versionhq import Tool, Agent, Task\n\nmy_tool = Tool(name=\"demo tool\", func=lambda x: \"demo func\")\n\nagent = Agent(\n    role=\"demo\",\n    goal=\"execute tools\",\n    func_calling_llm=\"gpt-4o\",\n    tools=[my_tool]\n)\n\ntask = Task(\n    description=\"execute tools\",\n    can_use_agent_tools=True, # if False, the agent's tools will NOT be called.\n    tool_res_as_final=True\n)\n\nres = task.execute(agent=agent)\nassert res == \"demo func\"\n</code></pre> <p>When the function calling LLM is not provided, we use the main model or default model <code>gpt-4o</code> .</p> <pre><code>from versionhq import Tool, Agent, Task\n\ndef demo_func(): return \"demo func\"\nmy_tool = Tool(name=\"demo tool\", func=demo_func)\n\nagent = Agent(\n    role=\"demo\",\n    goal=\"execute the given tools\",\n    llm=\"gemini-2.0\", # this model will be set as a function calling LLM.\n    tools=[my_tool]\n)\n\ntask = Task(\n    description=\"execute the given tools\",\n    can_use_agent_tools=True,\n    tool_res_as_final=True\n)\n\nres = task.execute(agent=agent)\nassert res.tool_output == \"demo func\"\n</code></pre> <pre><code>from versionhq import Tool, Agent, Task\n\nmy_tool = Tool(name=\"demo tool\", func=lambda x: \"demo func\")\n\nagent = Agent(\n    role=\"Demo Tool Handler\",\n    goal=\"execute tools\",\n    tools=[my_tool]\n)\n\ntask = Task(\n    description=\"execute tools\",\n    can_use_agent_tools=True,\n    tool_res_as_final=True\n)\n\nres = task.execute(agent=agent)\nassert res.tool_output == \"demo func\"\nassert agent.key in task.processed_agents\n</code></pre> <p>Function calling LLM</p> <p>By default, the agent will prioritize the given <code>func_calling_llm</code> over its main <code>llm</code> when it uses tools.</p> <p>When you build the agent, it will check if the model acutally supports function callings, and if not,  <code>func_calling_llm</code> will be switched to main <code>llm</code> or default model.</p> <p>If you want to see if the model of your choice supports function calling explicitly, run the following:</p> <pre><code>from versionhq.llm.model import LLM\nllm = LLM(model=\"&lt;MODEL_NAME_OF_YOUR_CHOICE&gt;\")\nres = llm._supports_function_calling()\n\nassert type(res) == bool\n</code></pre>","tags":["Utilities"]},{"location":"core/tool.html#2-add-tools-to-the-task","title":"2. Add tools to the task","text":"<p>This is a more explicit way to call tools on a specific task.</p> <p>Note your agent will NOT own the tool after the task execution.</p> <pre><code>from versionhq import Tool, ToolSet, Task, Agent\n\ndef random_func(message: str) -&gt; str:\n    return message + \"_demo\"\n\ntool = Tool(name=\"tool\", func=random_func)\n\ntool_set = ToolSet(\n    tool=tool,\n    kwargs=dict(message=\"empty func\")\n)\n\nagent = Agent(\n    role=\"Tool Handler\",\n    goal=\"execute tools\"\n)\n\ntask = Task(\n    description=\"execute the function\",\n    tools=[tool_set,], # use ToolSet to call args\n    tool_res_as_final=True\n)\n\nres = task.execute(agent=agent)\nassert res == \"empty func_demo\"\n</code></pre>","tags":["Utilities"]},{"location":"core/tool.html#decorator","title":"Decorator","text":"<p><code>[decorator]</code><code>@tool[name: str]: Callable[..., Any] -&gt; Any</code></p> <p>When you want to use an exsiting function as a tool, you can simply add a decorator to the function.</p> <pre><code>from versionhq.tool.decorator import tool\n\n@tool(\"demo\")\ndef my_tool(test_words: str) -&gt; str:\n\"\"\"Test a tool decorator.\"\"\"\nreturn test_words\n\nassert my_tool.name == \"demo\"\nassert \"Tool: demo\" in my_tool.description and \"'test_words': {'description': '', 'type': 'str'\" in my_tool.description\nassert my_tool.func(\"testing\") == \"testing\"\n</code></pre>","tags":["Utilities"]},{"location":"core/agent/index.html","title":"Agent","text":"<p><code>class</code> versionhq.agent.model.Agent <p>A Pydantic class to store an <code>Agent</code> object that handles <code>Task</code> execution.</p>","tags":["Agent Network"]},{"location":"core/agent/index.html#quick-start","title":"Quick Start","text":"<p>By defining its role and goal in a simple sentence, the AI agent will be set up to run on <code>gpt-4o</code> by default.</p> <p>Calling <code>.start()</code> method can start the agent operation and generate response in text and JSON formats stored in the <code>TaskOutput</code> object.</p> <pre><code>import versionhq as vhq\n\nagent = vhq.Agent(\n    role=\"Marketing Analyst\",\n    goal=\"Coping with price competition in saturated markets\"\n)\n\nres = agent.start(context=\"Planning a new campaign promotion starting this summer\")\n\nassert agent.id\nassert isinstance(res, vhq.TaskOutput)\nassert res.json\n</code></pre> <p>Ref. Task class / LLM class</p>","tags":["Agent Network"]},{"location":"core/agent/config.html","title":"Configuration","text":""},{"location":"core/agent/config.html#optimizing-model","title":"Optimizing Model","text":"<p>Model Optimization</p> <p><code>[var]</code><code>llm: Optional[str | LLM | Dict[str, Any]] = \"gpt-4o\"</code></p> <p>You can select a model or model provider that the agent will run on.</p> <p>By default, when the model provider name is provided, we will select the most cost-efficient model from the given provider.</p> <pre><code>import versionhq as vhq\n\nagent = vhq.Agent(role=\"Marketing Analyst\", llm=\"gemini-2.0\")\n</code></pre> <p>Other LLM Configuration</p> <p><code>[var]</code><code>llm_config: Optional[Dict[str, Any]] = None</code></p> <p>You can specify any other parameters that the agent needs to follow when they call the LLM. Else, the agent will follow the default settings given by the model provider.</p> <p>e.g. Expect longer context and form a short answer</p> <pre><code>import versionhq as vhq\n\nagent = vhq.Agent(\n    role=\"Marketing Analyst\",\n    respect_context_window=False,\n    max_execution_time=60,\n    max_rpm=5,\n    llm_config=dict(\n            temperature=1,\n            top_p=0.1,\n            n=1,\n            stop=\"answer\",\n            dummy=\"I am dummy\" # &lt;- invalid field will be ignored automatically.\n        )\n    )\n\nassert isinstance(agent.llm, vhq.LLM)\nassert agent.llm.llm_config[\"temperature\"] == 1\nassert agent.llm.llm_config[\"top_p\"] == 0.1\nassert agent.llm.llm_config[\"n\"] == 1\nassert agent.llm.llm_config[\"stop\"] == \"answer\"\n</code></pre>"},{"location":"core/agent/config.html#building-knowledge","title":"Building Knowledge","text":"<p>Knowlege Source</p> <p><code>[var]</code><code>knowledge_sources: Optional[List[KnowledgeSource]] = None</code></p> <p>You can add knowledge sources to the agent in the following formats:</p> <ul> <li>Plane text</li> <li>Excel file</li> <li>PPTX</li> <li>PDF</li> <li>CSV</li> <li>JSON</li> <li>HTML file</li> </ul> <p>The agent will run a query in the given knowledge source using the given context, then add the search results to the task prompt context.</p> <pre><code>import versionhq as vhq\n\ncontent = \"Kuriko's favorite color is gold, and she enjoy Japanese food.\"\nstring_source = vhq.StringKnowledgeSource(content=content)\n\nagent = vhq.Agent(\n    role=\"Information Agent\",\n    goal=\"Provide information based on knowledge sources\",\n    knowledge_sources=[string_source,]\n)\n\ntask = vhq.Task(\n    description=\"Answer the following question: What is Kuriko's favorite color?\"\n)\n\nres = task.execute(agent=agent)\nassert \"gold\" in res.raw  == True\n</code></pre> <ul> <li>Reference: <code>Knowledge</code> class</li> </ul>"},{"location":"core/agent/config.html#accessing-memories","title":"Accessing Memories","text":"<p>Store task execution results in memory</p> <p><code>[var]</code><code>with_memory: bool = False</code></p> <p>By turning on the with_memory val True, the agent will create and store the task output and contextualize the memory when they execute the task.</p> <pre><code>from versionhq.task.model import Agent\n\nagent = vhq.Agent(\n    role=\"Researcher\",\n    goal=\"You research about math.\",\n    with_memory=True\n)\n\nassert isinstance(agent.short_term_memory, vhq.ShortTermMemory)\nassert isinstance(agent.long_term_memory, vhq.LongTermMemory)\n</code></pre> <p>RAG Storage</p> <p>When the agent is not given any <code>memory_config</code> values, they will create <code>RAGStorage</code> to store memory:</p> <pre><code>RAGStorage(\n    type=\"stm\", # short-term memory\n    allow_reset=True, # default = True. Explicitly mentioned.\n    embedder_config=None,\n    agents=[agent,]\n)\n</code></pre> <p>MEM0 Storage</p> <ul> <li>Reference: <code>Memory</code> class</li> </ul>"},{"location":"core/agent/config.html#updating-existing-agents","title":"Updating Existing Agents","text":"<p>Model configuration</p> <p><code>[var]</code><code>config: Optional[Dict[str, Any]] = None</code></p> <p>You can create an agent by using model config parameters instead.</p> <p>e.g. Using config val</p> <pre><code>import versionhq as vhq\n\nagent = vhq.Agent(\n    config=dict(\n        role=\"Marketing Analyst\",\n        goal=\"Coping with increased price competition in saturated markets.\",\n    )\n)\n</code></pre> <p>This is the same as the following:</p> <pre><code>import versionhq as vhq\n\nagent = vhq.Agent(\n    role=\"Marketing Analyst\",\n    goal=\"Coping with price competition in saturated markets.\",\n)\n</code></pre> <p>Updating existing agents</p> <p><code>[class method]</code><code>update(self, **kwargs) -&gt; Self</code></p> <p>You can update values of exsiting agents using <code>update</code> class method.</p> <p>This class method will safely trigger some setups that needs to be run before the agent start executing tasks.</p> <pre><code>import versionhq as vhq\n\nagent = vhq.Agent(\n    role=\"Marketing Analyst\",\n    goal=\"Coping with price competition in saturated markets\"\n)\n\ntool = vhq.Tool(func=lambda x: x)\nagent.update(\n    tools=[tool],\n    goal=\"my new goal\", # updating the goal (this will trigger updating the developer_prompt.)\n    max_rpm=3,\n    knowledge_sources=[\"testing\", \"testing2\"], # adding knowledge sources (this will trigger the storage creation.)\n    memory_config={\"user_id\": \"0000\"},\n    llm=\"gemini-2.0\", # Updating model (The valid llm_config for the new model will be inherited.)\n    use_developer_prompt=False,\n    dummy=\"I am dummy\" # &lt;- Invalid field will be automatically ignored.\n)\n</code></pre>"},{"location":"core/agent/task-handling.html","title":"Task Handling","text":""},{"location":"core/agent/task-handling.html#prompt-engineering","title":"Prompt Engineering","text":"<p>Developer Prompt</p> <p><code>[var]</code><code>backstory: Optional[str] = TEMPLATE_BACKSTORY</code> <p>Backstory will be drafted automatically using the given role, goal and other values in the Agent model, and converted into the developer prompt when the agent executes the task.</p> <p>Backstory template (full) for auto drafting:</p> <pre><code>BACKSTORY_FULL=\"\"\"You are an expert {role} highly skilled in {skills}. You have abilities to query relevant information from the given knowledge sources and use tools such as {tools}. Leveraging these, you will identify competitive solutions to achieve the following goal: {goal}.\"\"\"\n</code></pre> <p>For example, the following agent\u2019s backstory will be auto drafted using a simple template.</p> <pre><code>import versionhq as vhq\n\nagent = vhq.Agent(\n    role=\"Marketing Analyst\",\n    goal=\"Coping with price competition in saturated markets\"\n)\n\nassert agent.backstory == \"You are an expert marketing analyst with relevant skills and abilities to query relevant information from the given knowledge sources. Leveraging these, you will identify competitive solutions to achieve the following goal: coping with price competition in saturated markets.\"\n</code></pre> <p>You can also specify your own backstory by simply adding the value to the backstory field of the Agent model:</p> <pre><code>import versionhq as vhq\n\nagent = vhq.Agent(\n    role=\"Marketing Analyst\",\n    goal=\"Coping with increased price competition in saturated markets.\",\n    backstory=\"You are a marketing analyst for a company in a saturated market. The market is becoming increasingly price-competitive, and your company's profit margins are shrinking. Your primary goal is to develop and implement strategies to help your company maintain its market share and profitability in this challenging environment.\"\n)\n\nassert agent.backstory == \"You are a marketing analyst for a company in a saturated market. The market is becoming increasingly price-competitive, and your company's profit margins are shrinking. Your primary goal is to develop and implement strategies to help your company maintain its market share and profitability in this challenging environment.\"\n</code></pre> <p><code>[var]</code><code>use_developer_prompt: [bool] = True</code></p> <p>You can turn off the system prompt by setting <code>use_developer_prompt</code> False. In this case, the backstory is ignored when the agent call the LLM.</p> <pre><code>import versionhq as vhq\n\nagent = vhq.Agent(\n    role=\"Marketing Analyst\",\n    goal=\"Coping with increased price competition in saturated markets.\",\n    use_developer_prompt=False # default - True\n)\n</code></pre>"},{"location":"core/agent/task-handling.html#executing-tasks","title":"Executing Tasks","text":"<p>Delegation</p> <p><code>[var]</code><code>allow_delegation: [bool] = False</code></p> <p>When the agent is occupied with other tasks or not capable enough to the given task, you can delegate the task to another agent or ask another agent for additional information. The delegated agent will be selected based on nature of the given task and/or tool.</p> <pre><code>import versionhq as vhq\n\nagent = vhq.Agent(\n    role=\"Marketing Analyst\",\n    goal=\"Coping with increased price competition in saturated markets.\",\n    allow_delegation=True\n)\n</code></pre> <p>Max Retry Limit</p> <p><code>[var]</code><code>max_retry_limit: Optional[int] = 2</code></p> <p>You can define how many times the agent can retry the execution under the same given conditions when it encounters an error.</p> <pre><code>import versionhq as vhq\n\nagent = vhq.Agent(\n    role=\"Marketing Analyst\",\n    goal=\"Coping with increased price competition in saturated markets.\",\n    max_retry_limit=3\n)\n</code></pre> <p>Maximum Number of Iterations (maxit)</p> <p><code>[var]</code><code>maxit: Optional[int] = 25</code></p> <p>You can also define the number of loops that the agent will run after it encounters an error.</p> <p>i.e., The agent will stop the task execution after the 30<sup>th</sup> loop.</p> <pre><code>import versionhq as vhq\n\nagent = vhq.Agent(\n    role=\"Marketing Analyst\",\n    goal=\"Coping with increased price competition in saturated markets.\",\n    maxit=30 # default = 25\n)\n</code></pre> <p>Context Window</p> <p><code>[var]</code><code>respect_context_window: [bool] = True</code></p> <p>A context window determines the amount of text that the model takes into account when generating a response.</p> <p>By adjusting the context window, you can control the level of context the model considers while generating the output. A smaller context window focuses on immediate context, while a larger context window provides a broader context.</p> <p>By default, the agent will follow the 80% rule - where they only use 80% of the context window limit of the LLM they run on.</p> <p>You can turn off this rule by setting <code>respect_context_window</code> False to have larger context window.</p> <p>Max Tokens</p> <p><code>[var]</code><code>max_tokens: Optional[int] = None</code></p> <p>Max tokens defines the maximum number of tokens in the generated response. Tokens can be thought of as the individual units of text, which can be words or characters.</p> <p>By default, the agent will follow the default max_tokens of the model, but you can specify the max token to limit the length of the generated output.</p> <p>Maximum Execution Time</p> <p><code>[var]</code><code>max_execution_times: Optional[int] = None</code></p> <p>The maximum amount of wall clock time to spend in the execution loop.</p> <p>By default, the agent will follow the default setting of the model.</p> <p>Maximum RPM (Requests Per Minute)</p> <p><code>[var]</code><code>max_rpm: Optional[int] = None</code></p> <p>The maximum number of requests that the agent can send to the LLM.</p> <p>By default, the agent will follow the default setting of the model. When the value is given, we let the model sleep for 60 seconds when the number of executions exceeds the maximum requests per minute.</p>"},{"location":"core/agent/task-handling.html#callbacks","title":"Callbacks","text":"<p><code>[var]</code><code>callbacks: Optional[List[Callable]] = None</code></p> <p>You can add callback functions that the agent will run after executing any task.</p> <p>By default, raw response from the agent will be added to the arguments of the callback function.</p> <p>e.g. Format a response after executing the task:</p> <pre><code>import json\nfrom typing import Dict, Any\n\nimport versionhq as vhq\n\ndef format_response(res: str = None) -&gt; str | Dict[str, Any]:\n    try:\n        r = json.dumps(eval(res))\n        formatted_res = json.loads(r)\n        return formatted_res\n    except:\n        return res\n\nagent = vhq.Agent(\n    role=\"Marketing Analyst\",\n    goal=\"Coping with increased price competition in saturated markets.\",\n    callbacks=[format_response]\n)\nres = agent.start()\n\nassert res.callback_output\n</code></pre> <p>Multiple callbacks to call</p> <p>The callback functions are called in order of the list index referring to the task response and response from the previous callback functions by default.</p> <p>e.g. Validate an initial response from the assigned agent, and format the response.</p> <pre><code>import json\nfrom typing import Dict, Any\n\nimport versionhq as vhq\n\ndef assessment(res: str) -&gt; str:\n    try:\n        sub_agent = vhq.Agent(role=\"Validator\", goal=\"Validate the given solutions.\")\n        task = vhq.Task(\n            description=f\"Assess the given solution based on feasibilities and fits to client's strategies, then refine the solution if necessary.\\nSolution: {res}\"\n        )\n        r = task.sync_execute(agent=sub_agent)\n        return r.raw\n\n    except:\n        return res\n\ndef format_response(res: str = None) -&gt; str | Dict[str, Any]:\n    try:\n        r = json.dumps(eval(res))\n        formatted_res = json.loads(r)\n        return formatted_res\n    except:\n        return res\n\nagent = vhq.Agent(\n    role=\"Marketing Analyst\",\n    goal=\"Build solutions to address increased price competition in saturated markets\",\n    callbacks=[assessment, format_response] # add multiple funcs as callbacks - executed in order of index\n)\n</code></pre>"},{"location":"core/agent-network/index.html","title":"Agent Network","text":"<p><code>class</code> versionhq.agent_network.model.AgentNetwork <p>A Pydantic class to store <code>AgentNetwork</code> objects that handle multiple agent formations for the task execution.</p> <p>You can specify a desired formation or allow the agents to determine it autonomously (default).</p> Solo Agent Supervising Squad Random Formation Usage <ul><li>A single agent with tools, knowledge, and memory.</li><li>When self-learning mode is on - it will turn into Random formation.</li></ul> <ul><li>Leader agent gives directions, while sharing its knowledge and memory.</li><li>Subordinates can be solo agents or networks.</li></ul> <ul><li>Share tasks, knowledge, and memory among network members.</li></ul> <ul><li>A single agent handles tasks, asking help from other agents without sharing its memory or knowledge.</li></ul> Use case An email agent drafts promo message for the given audience. The leader agent strategizes an outbound campaign plan and assigns components such as media mix or message creation to subordinate agents. An email agent and social media agent share the product knowledge and deploy multi-channel outbound campaign. 1. An email agent drafts promo message for the given audience, asking insights on tones from other email agents which oversee other clusters. 2. An agent calls the external agent to deploy the campaign.","tags":["Agent Network"]},{"location":"core/agent-network/index.html#quick-start","title":"Quick Start","text":"<p>By default, lead agents will determine the best network formation autonomously based on the given task and its goal.</p> <p>Calling <code>.launch()</code> method can start executing tasks and generate a tuple of response as a <code>TaskOutput</code> object and <code>TaskGraph</code> object.</p> <pre><code>import versionhq as vhq\n\nnetwork = vhq.form_agent_network(\n  task=f\"create a promo plan to attract a client\",\n  expected_outcome='media mix, key messages, and CTA targets.'\n)\n\nres, tg = network.launch()\n\nassert isinstance(res, vhq.TaskOutput)\nassert isinstance(tg, vhq.TaskGraph)\n</code></pre> <p>Ref. TaskOutput / TaskGraph  class.</p> <p>Visit Playground.</p>","tags":["Agent Network"]},{"location":"core/agent-network/config.html","title":"Configuring","text":"","tags":["Agent Network"]},{"location":"core/agent-network/config.html#adding-members","title":"Adding Members","text":"<p><code>class</code> versionhq.agent_network.model.Member <p>You can simply add an agent as a member using <code>members</code> field.</p> <pre><code>import versionhq as vhq\n\nnetwork = vhq.AgentNetwork(\n    members=[\n        vhq.Member(\n            agent=vhq.Agent(role=\"new member\", goal=\"work in the network\"),\n            is_manager=False  # explicitly mentioned. Setting `True` makes this member a manager of the network.\n        ),\n    ]\n)\nassert isinstance(network.members[0].agent, vhq.Agent)\n</code></pre> <p>Ref. <code>Member</code> class.</p>","tags":["Agent Network"]},{"location":"core/agent-network/config.html#changing-formation","title":"Changing Formation","text":"<p>The formation of network members will be automatically assigned based on the task goals, but you can explicitly define it by using <code>formation</code> field.</p> <pre><code>import versionhq as vhq\n\nnetwork = vhq.AgentNetwork(\n  members=[\n    vhq.Member(agent=vhq.Agent(role=\"member 1\", goal=\"work in the network\")),\n    vhq.Member(agent=vhq.Agent(role=\"member 2\", goal=\"work in the network\")),\n    vhq.Member(agent=vhq.Agent(role=\"member 3\", goal=\"work in the network\")),\n  ],\n  formation=vhq.Formation.SQUAD,\n)\n\nassert network.formation == vhq.Formation.SQUAD\n</code></pre> <p>Ref. Enum <code>Formation</code></p>","tags":["Agent Network"]},{"location":"core/agent-network/config.html#task-handling","title":"Task Handling","text":"<p>The class method <code>.launch()</code> will automatically decide the best task handling process and execute the tasks accordingly.</p> <pre><code>import versionhq as vhq\n\nnetwork = vhq.AgentNetwork(\n  members=[\n      vhq.Member(agent=vhq.Agent(role=\"member 1\", goal=\"work in the network\"), tasks=[vhq.Task(description=\"Run a demo 1\")]),\n      vhq.Member(agent=vhq.Agent(role=\"member 2\", goal=\"work in the network\"), tasks=[vhq.Task(description=\"Run a demo\")]),\n      vhq.Member(agent=vhq.Agent(role=\"member 3\", goal=\"work in the network\")),\n  ],\n)\n\nres, tg = network.launch()\n\nassert isinstance(res, vhq.TaskOutput)\nassert isinstance(tg, vhq.TaskGraph)\n</code></pre> <p>You can also specify the process using <code>process</code> field.</p> <pre><code>import versionhq as vhq\n\nnetwork = vhq.AgentNetwork(\n  members=[\n      vhq.Member(agent=vhq.Agent(role=\"member 1\", goal=\"work in the network\"), tasks=[vhq.Task(description=\"Run a demo 1\")]),\n      vhq.Member(agent=vhq.Agent(role=\"member 2\", goal=\"work in the network\"), tasks=[vhq.Task(description=\"Run a demo 2\")]),\n      vhq.Member(agent=vhq.Agent(role=\"member 3\", goal=\"work in the network\")),\n  ],\n  process=vhq.TaskHandlingProcess.CONSENSUAL,\n  consent_trigger=lambda x: True, # consent trigger event is a MUST for TaskHandlingProcess.CONSENSUAL\n)\n\nres, tg = network.launch()\n\nassert isinstance(res, vhq.TaskOutput)\nassert isinstance(tg, vhq.TaskGraph)\n</code></pre> <p>Ref. Enum <code>TaskHandlingProcess</code></p>","tags":["Agent Network"]},{"location":"core/agent-network/form.html","title":"Generating","text":"<p>You can generate an <code>AgentNetwork</code> by using <code>form_agent_network</code> method with a concise <code>task</code> description and <code>expected_outcome</code> args.</p> <pre><code>import versionhq as vhq\n\nnetwork = vhq.form_agent_network(\n    task=\"Find the best trip destination this summer.\",\n    expected_outcome=\"a list of destinations and why it's suitable\",\n    context=\"planning a suprise trip for my friend\", # optional\n)\n\nassert isinstance(network, vhq.AgentNetwork)\nassert network.members # auto-generated agents as network members\nassert network.tasks # auto-defined sub-tasks to achieve the main task goal\n</code></pre> <p>Strucured Output</p> <p>To generate structured output, you can add a JSON dict or Pydantic class as <code>expected_outcome</code> args instead of plane text.</p> <pre><code>import versionhq as vhq\nfrom pydantic import BaseModel\n\nclass Outcome(BaseModel):\n    destinations: list[str]\n    why_suitable: list[str]\n\n\nnetwork = vhq.form_agent_network(\n    task=\"Find the best trip destination this summer.\",\n    expected_outcome=Outcome,\n    context=\"planning a suprise trip for my friend\", # optional\n)\n\nassert isinstance(network, vhq.AgentNetwork)\nassert network.members\nassert network.tasks\n</code></pre> <p>Agents</p> <p>You can use <code>agents</code> args to add existing agents to the network.</p> <pre><code>import versionhq as vhq\nfrom pydantic import BaseModel\n\nmy_agent = vhq.Agent(\n    role=\"Travel Agent\",\n    goal=\"select best trip destination\",\n    knowledge_sources=[\".....\",\"url1\",]\n)\n\nclass Outcome(BaseModel):\n    destinations: list[str]\n    why_suitable: list[str]\n\nnetwork = vhq.form_agent_network(\n    task=\"Find the best trip destination this summer.\",\n    expected_outcome=Outcome,\n    context=\"planning a suprise trip for my friend\",\n    agents=[my_agent,]\n)\n\nassert isinstance(network, vhq.AgentNetwork)\nassert [member for member in network.members if member.agent == my_agent]\nassert network.tasks\n</code></pre> <p>Formation</p> <p>Similar to <code>agents</code>, you can define <code>formation</code> args to specify the network formation:</p> <pre><code>import versionhq as vhq\nfrom pydantic import BaseModel\n\nmy_agent = vhq.Agent(\n    role=\"Travel Agent\",\n    goal=\"select best trip destination\",\n    knowledge_sources=[\".....\",\"url1\",]\n)\n\nclass Outcome(BaseModel):\n    destinations: list[str]\n    why_suitable: list[str]\n\nnetwork = vhq.form_agent_network(\n    task=\"Find the best trip destination this summer.\",\n    expected_outcome=Outcome,\n    context=\"planning a suprise trip for my friend\",\n    agents=[my_agent,],\n    formation=vhq.Formation.SUPERVISING\n)\n\nassert isinstance(network, vhq.AgentNetwork)\nassert [member for member in network.members if member.agent == my_agent]\nassert network.tasks\nassert network.formation == vhq.Formation.SUPERVISING\n</code></pre> <p>Ref. Enum Formation</p>","tags":["Agent Network"]},{"location":"core/agent-network/ref.html","title":"Reference","text":"","tags":["Agent Network"]},{"location":"core/agent-network/ref.html#class-agentnetwork","title":"Class <code>AgentNetwork</code>","text":"","tags":["Agent Network"]},{"location":"core/agent-network/ref.html#variable","title":"Variable","text":"Variable Data Type Default Description <code>id</code> UUID4 uuid.uuid4() Stores auto-generated ID as identifier. Not editable. <code>name</code> str None Stores a name of the network. <code>members</code> List[<code>Member</code>] list() Stores a list of <code>Member</code> objects. <code>formation</code> <code>Formation</code> None Stores <code>Formation</code> enum. <code>should_reform</code> bool False Whether to reform the network during the activation. <code>network_tasks</code> List[<code>Task</code>] list() A list of <code>Task</code> objects unassigned to any network members. <code>prompt_file</code> str None Absolute file path to the prompt file w/ JSON formatted prompt. <code>process</code> <code>TaskHandlingProcess</code> TaskHandlingProcess.SEQUENTIAL Enum of the task handling process. <code>consent_trigger</code> Callable[..., Any] None A trigger event (func) for consentual processing. <code>pre_launch_callbacks</code> List[Callable[..., Any]] list() Stores callbacks to run before the network launch. <code>post_launch_callbacks</code> List[Callable[..., Any]] list() Stores callbacks to run after the network launch. <code>step_callbacks</code> Callable[..., Any] None Stores callbacks to run at every step of each member agent takes during the activation. <code>cache</code> bool True Whether to store cache. <code>execution_logs</code> List[Dict[str, Any]] list() Stores a list of execution logs of all the tasks in the network.","tags":["Agent Network"]},{"location":"core/agent-network/ref.html#class-methods","title":"Class Methods","text":"Method Params Returns Description <code>launch</code> kwargs_pre: Optional[Dict[str, str]] = None  kwargs_post: Optional[Dict[str, Any]] = None  start_index: int = None Tuple[TaskOutput, TaskGraph]: Core method to launch the network and execute tasks","tags":["Agent Network"]},{"location":"core/agent-network/ref.html#properties","title":"Properties","text":"Property Returns Description <code>key</code> str Unique identifier. <code>managers</code> List[InstanceOf[<code>Member</code>]] A list of manager members. <code>manager_tasks</code> List[InstanceOf[<code>Task</code>]] A list of tasks handled by managers. <code>tasks</code> List[InstanceOf[<code>Task</code>]] All the tasks in the network. <code>unassigned_member_tasks</code> List[InstanceOf[<code>Task</code>]] Unassigned member-level tasks.","tags":["Agent Network"]},{"location":"core/agent-network/ref.html#class-member","title":"Class <code>Member</code>","text":"","tags":["Agent Network"]},{"location":"core/agent-network/ref.html#variable_1","title":"Variable","text":"Variable Data Type Default Description <code>agent</code> InstanceOf[<code>Agent</code>] None Agent as a member <code>is_manager</code> bool False Whether the member is a manager. <code>can_share_knowledge</code> bool True Whether the member can share its knowledge among the other network members. <code>can_share_memory</code> bool True Whether the member can share its memories among the other network members. <code>tasks</code> List[InstanceOf[<code>Task</code>]] list() Assinged tasks.","tags":["Agent Network"]},{"location":"core/agent-network/ref.html#properties_1","title":"Properties","text":"Property Returns Description <code>is_idling</code> bool Whether it has unprocessed assgined task/s","tags":["Agent Network"]},{"location":"core/agent-network/ref.html#class-agent","title":"Class <code>Agent</code>","text":"","tags":["Agent Network"]},{"location":"core/agent-network/ref.html#variables","title":"Variables","text":"Variable Data Type Default Description <code>id</code> UUID4 uuid.uuid4() Stores auto-generated ID as identifier. Not editable. <code>role</code> str None Stores a role of the agent. <code>goal</code> str None Stores a goal of the agent. <code>backstory</code> str None Stores backstory of the agent. Utilized as system prompt. <code>self_learn</code> bool False Whether to self-learn <code>tools</code> List[InstanceOf[<code>ToolSet</code>] | Type[<code>Tool</code>]] None Stores tools to be used when executing a task. <code>knowledge_sources</code> List[<code>BaseKnowledgeSource</code> | Any] None Stores knowledge sources in text, file path, or url. <code>embedder_config</code> Dict[str, Any] None Stores embedding configuration for storing knowledge sources. <code>with_memory</code> bool False Whether to store tasks and results in memory. <code>memory_config</code> Dict[str, Any] None Stores configuration of the memory. <code>short_term_memory</code> InstanceOf[<code>ShortTermMemory</code>] None Stores <code>ShortTermMemory</code> object. <code>long_term_memory</code> InstanceOf[<code>LongTermMemory</code>] None Stores <code>LongTermMemory</code> object. <code>user_memory</code> InstanceOf[<code>UserMemory</code>] None Stores <code>UserMemory</code> object. <code>use_developer_prompt</code> bool True Whether to use the system (developer) prompt when calling the model. <code>developer_promt_template</code> str None File path to the prompt template. <code>user_promt_template</code> str None File path to the prompt template. <code>networks</code> List[Any] list() Stores a list of agent networks that the agent belongs to. <code>allow_delegation</code> bool False Whether the agent can delegate assinged tasks to another agent. <code>max_retry_limit</code> int 2 Maximum number of retries when the task execution failed. <code>maxit</code> int 25 Maximum number of total optimization loops during error handling. <code>callbacks</code> List[Callabale] None Stores a list of step callback functions. <code>llm</code> str | InstanceOf[<code>LLM</code>] | Dict[str, Any] None Stores the main LLM. <code>func_calling_llm</code> str | InstanceOf[<code>LLM</code>] | Dict[str, Any] None Stores the function calling LLM. <code>respect_context_window</code> bool True Whether to follow the main model's maximum context window size. <code>max_execution_time</code> int None Stores maximum execution time in seconds. <code>max_rpm</code> int None Stores maximum number of requests per minute. <code>llm_config</code> Dict[str, Any] None Stores configuration of <code>LLM</code> object. <code>config</code> Dict[str, Any] None Stores model config.","tags":["Agent Network"]},{"location":"core/agent-network/ref.html#class-methods_1","title":"Class Methods","text":"Method Params Returns Description <code>update</code> **kwargs: Any Self Updates agents with given kwargs. Invalid keys will be ignored. <code>start</code> context: Any = None  tool_res_as_final: bool = False <code>TaskOutput</code> | None Starts to operate the agent. <code>execute_task</code> task: [Task]  context: Any = None Tuple[str, str, Any, UsageMetrics] Returns response from the model in plane text format.","tags":["Agent Network"]},{"location":"core/agent-network/ref.html#properties_2","title":"Properties","text":"Property Returns Description <code>key</code> str Unique identifier of the agent using its ID and sanitized role.","tags":["Agent Network"]},{"location":"core/agent-network/ref.html#enum-formation","title":"ENUM <code>Formation</code>","text":"<pre><code>class Formation(IntEnum):\n    SOLO = 1\n    SUPERVISING = 2\n    SQUAD = 3\n    RANDOM = 4\n    HYBRID = 10\n</code></pre>","tags":["Agent Network"]},{"location":"core/agent-network/ref.html#enum-taskhandlingprocess","title":"ENUM <code>TaskHandlingProcess</code>","text":"<pre><code>class TaskHandlingProcess(IntEnum):\n    HIERARCHY = 1\n    SEQUENTIAL = 2\n    CONSENSUAL = 3\n</code></pre>","tags":["Agent Network"]},{"location":"core/llm/index.html","title":"LLM","text":"<p><code>class</code> versionhq.llm.model.LLM <p>A Pydantic class to store LLM objects and its task handling rules.</p> <p>You can specify a model and integration platform from the list. Else, we'll use <code>gemini</code> or <code>gpt</code> via <code>LiteLLM</code> by default.</p> <p>List of available models</p> <pre><code>\"openai\": [\n    \"gpt-4.1\",\n    \"gpt-4\",\n    \"gpt-4o\",\n    \"gpt-4o-mini\",\n    \"o3-mini\",\n    \"o3-mini-2025-01-31\",\n    \"o1-mini\",\n    \"o1-preview\",\n],\n\"gemini\": [\n    \"gemini/gemini-2.5-pro-exp-03-25\",\n    \"gemini/gemini-2.0-flash\",\n    \"gemini/gemini-2.0-flash-thinking-exp\",\n    \"gemini/gemini-2.0-flash-lite-preview-02-05\",\n    \"gemini/gemini-2.0-flash-exp\",\n],\n\"anthropic\": [\n    \"claude-3-7-sonnet-latest\",\n    \"claude-3-5-haiku-latest\",\n    \"claude-3-5-sonnet-latest\",\n    \"claude-3-opus-latest\",\n],\n\"openrouter\": [\n    \"openrouter/deepseek/deepseek-r1\",\n    \"openrouter/qwen/qwen-2.5-72b-instruct\",\n    \"openrouter/google/gemini-2.0-flash-001\",\n    \"openrouter/mistralai/mistral-large\",\n    \"openrouter/cohere/command-r-plus\",\n    \"openrouter/databricks/dbrx-instruct\",\n],\n\"bedrock\": [\n    \"bedrock/converse/us.meta.llama3-3-70b-instruct-v1:0\",\n    \"bedrock/us.meta.llama3-2-1b-instruct-v1:0\",\n    \"bedrock/us.meta.llama3-2-3b-instruct-v1:0\",\n    \"bedrock/us.meta.llama3-2-11b-instruct-v1:0\",\n    \"bedrock/us.meta.llama3-2-90b-instruct-v1:0\",\n    \"bedrock/mistral.mistral-7b-instruct-v0:2\",\n    \"bedrock/mistral.mixtral-8x7b-instruct-v0:1\",\n    \"bedrock/mistral.mistral-large-2407-v1:0\",\n    \"bedrock/amazon.titan-text-lite-v1\",\n    \"bedrock/amazon.titan-text-express-v1\",\n    \"bedrock/amazon.titan-text-premier-v1:0\",\n    \"bedrock/cohere.command-r-plus-v1:0\",\n    \"bedrock/cohere.command-r-v1:0\",\n    \"bedrock/cohere.command-text-v14\",\n    \"bedrock/cohere.command-light-text-v14\",\n],\n\"azure\": [\n    \"azure/DeepSeek-V3-0324\",\n    \"azure/DeepSeek-R1\",\n    \"azure/Llama-3.3-70B-Instruct\",\n    \"azure/Llama-3.2-11B-Vision-Instruct\",\n    \"azure/Meta-Llama-3.1-405B-Instruct\",\n    \"azure/Meta-Llama-3.1-8B-Instruct\",\n    \"azure/Llama-3.2-1B-Instruct\",\n    \"azure/Meta-Llama-3.1-70B\",\n    \"azure/Meta-Llama-3.1-8B\",\n    \"azure/Llama-3.2-3B-Instruct\",\n    \"azure/Meta-Llama-3-8B-Instruct\",\n    \"azure/Meta-Llama-3.1-70B-Instruct\",\n    \"azure/Llama-3.2-90B-Vision-Instruct\",\n    \"azure/Llama-3.2-3B\",\n    \"azure/Llama-3.2-1B\",\n    \"azure/mistral-large-latest\",\n    \"azure/mistral-large-2402\",\n    \"azure/command-r-plus\",\n    \"azure/o3-mini-2025-01-31\",\n    \"azure/o3-mini\",\n    \"azure/o1-mini\",\n    \"azure/Phi-4-mini-instruct\",\n    \"azure/Phi-4-multimodal-instruct\",\n    \"azure/Mistral-Large-2411\",\n    \"azure/Mistral-small\"\n    \"azure/mistral-small-2503\",\n    \"azure/Ministral-3B\",\n    \"azure/mistralai-Mixtral-8x22B-v0-1\"\n    \"azure/Cohere-rerank-v3.5\",\n],\n\"azure_ai\": [\n    \"azure/DeepSeek-V3-0324\",\n    \"azure_ai/DeepSeek-R1\",\n    \"azure_ai/Llama-3.3-70B-Instruct\",\n    \"azure_ai/Llama-3.2-11B-Vision-Instruct\",\n    \"azure_ai/Meta-Llama-3.1-405B-Instruct\",\n    \"azure_ai/Meta-Llama-3.1-8B-Instruct\",\n    \"azure_ai/Llama-3.2-1B-Instruct\",\n    \"azure_ai/Meta-Llama-3.1-70B\",\n    \"azure_ai/Meta-Llama-3.1-8B\",\n    \"azure_ai/Llama-3.2-3B-Instruct\",\n    \"azure_ai/Meta-Llama-3-8B-Instruct\",\n    \"azure_ai/Meta-Llama-3.1-70B-Instruct\",\n    \"azure_ai/Llama-3.2-90B-Vision-Instruct\",\n    \"azure_ai/Llama-3.2-3B\",\n    \"azure_ai/Llama-3.2-1B\",\n    \"azure_ai/mistral-large-latest\",\n    \"azure_ai/mistral-large-2402\",\n    \"azure_ai/command-r-plus\",\n    \"azure_ai/o3-mini-2025-01-31\",\n    \"azure_ai/o3-mini\",\n    \"azure_ai/o1-mini\",\n    \"azure_ai/Phi-4-mini-instruct\",\n    \"azure_ai/Phi-4-multimodal-instruct\",\n    \"azure_ai/Mistral-Large-2411\",\n    \"azure_ai/Mistral-small\"\n    \"azure_ai/mistral-small-2503\",\n    \"azure_ai/Ministral-3B\",\n    \"azure_ai/mistralai-Mixtral-8x22B-v0-1\"\n    \"azure_ai/Cohere-rerank-v3.5\",\n],\n\"huggingface\": [\n    \"huggingface/qwen/qwen2.5-VL-72B-Instruct\",\n]\n</code></pre>","tags":["Agent Network"]},{"location":"core/task/index.html","title":"Task","text":"<p><code>class</code> versionhq.task.model.Task <p>A Pydantic class to store and manage information for individual tasks, including their assignment to agents or agent networks, and dependencies via a node-based system that tracks conditions and status.</p> <p>Ref. Node / Edge / TaskGraph class</p>","tags":["Task Graph"]},{"location":"core/task/index.html#quick-start","title":"Quick Start","text":"<p>Create a task by defining its description in one simple sentence. The <code>description</code> will be used in the prompt later.</p> <p>Each task will be assigned a unique ID as an identifier.</p> <pre><code>import versionhq as vhq\n\ntask = vhq.Task(description=\"MY AMAZING TASK\")\n\nimport uuid\nassert uuid.UUID(str(task.id), version=4)\n</code></pre> <p>And you can simply execute the task by calling <code>.execute()</code> function.</p> <pre><code>import versionhq as vhq\n\ntask = vhq.Task(description=\"MY AMAZING TASK\")\nres = task.execute()\n\nassert isinstance(res, vhq.TaskOutput) # Generates TaskOutput object\nassert res.raw and res.json # By default, TaskOutput object stores output in plane text and json formats.\nassert task.processed_agents is not None # Agents will be automatically assigned to the given task.\n</code></pre>","tags":["Task Graph"]},{"location":"core/task/index.html#evaluating","title":"Evaluating","text":"<p><code>[var]</code><code>should_evaluate: bool = False</code></p> <p><code>[var]</code><code>eval_criteria: Optional[List[str]] = list()</code></p> <p>You can turn on customized evaluations using the given criteria.</p> <p>Refer TaskOutput class for details.</p>","tags":["Task Graph"]},{"location":"core/task/evaluation.html","title":"Evaluation","text":"<p><code>class</code> versionhq.task.evaluate.Evaluation <p>A Pydantic class to store conditions and results of the evaluation.</p>","tags":["Task Graph"]},{"location":"core/task/evaluation.html#variables","title":"Variables","text":"Variable Data Type Default Nullable Description <code>items</code> List[InstanceOf[EvaluationItem]] list() - Stores evaluation items. <code>eval_by</code> Any None True Stores an agent evaluated the output.","tags":["Task Graph"]},{"location":"core/task/evaluation.html#property","title":"Property","text":"Property Returns Description <code>aggregate_score</code> float Calucurates weighted average eval scores of the task output. <code>suggestion_summary</code> str Returns summary of the suggestions.","tags":["Task Graph"]},{"location":"core/task/evaluation.html#evaluationitem","title":"EvaluationItem","text":"<p><code>class</code> versionhq.task.evaluate.EvaluationItem","tags":["Task Graph"]},{"location":"core/task/evaluation.html#variables_1","title":"Variables","text":"Variable Data Type Default Nullable Description <code>criteria</code> str None False Stores evaluation criteria given by the client. <code>suggestion</code> str None True Stores suggestion on improvement from the evaluator agent. <code>score</code> float None True Stores the score on a 0 to 1 scale.","tags":["Task Graph"]},{"location":"core/task/evaluation.html#usage","title":"Usage","text":"<p>Evaluator agents will evaluate the task output based on the given criteria, and store the results in the <code>TaskOutput</code> object.</p> <pre><code>import versionhq as vhq\nfrom pydantic import BaseModel\n\nclass CustomOutput(BaseModel):\n    test1: str\n    test2: list[str]\n\ntask = vhq.Task(\n    description=\"Research a topic to teach a kid aged 6 about math.\",\n    response_schema=CustomOutput,\n    should_evaluate=True, # triggers evaluation\n    eval_criteria=[\"uniquness\", \"audience fit\",],\n\n)\nres = task.execute()\n\nassert isinstance(res.evaluation, vhq.Evaluation)\nassert [item for item in res.evaluation.items if item.criteria == \"uniquness\" or item.criteria == \"audience fit\"]\nassert res.evaluation.aggregate_score is not None\nassert res.evaluation.suggestion_summary is not None\n</code></pre> <p>An <code>Evaluation</code> object provides scores for the given criteria.</p> <p>For example, it might indicate a <code>uniqueness</code> score of 0.56, an <code>audience fit</code> score of 0.70, and an <code>aggregate score</code> of 0.63.</p>","tags":["Task Graph"]},{"location":"core/task/reference.html","title":"Reference","text":""},{"location":"core/task/reference.html#class-task","title":"Class <code>Task</code>","text":""},{"location":"core/task/reference.html#variables","title":"Variables","text":"Variable Data Type Default Description <code>id</code> UUID uuid.uuid4() Stores task <code>id</code> as an identifier. <code>name</code> Optional[str] None Stores a task name (Inherited as <code>node</code> identifier if the task is dependent) <code>description</code> str None Required field to store a concise task description <code>response_schema</code> Optional[Type[BaseModel] | List[ResponseField]] None Response schema for structured output. <code>tools</code> Optional[List[Any]] None Tools, tool sets, or RAG tools <code>can_use_agent_tools</code> bool True Whether to use the agent tools <code>tool_res_as_final</code> bool False Whether to make a tool output as a final response from the agent <code>is_multimodal</code> bool False Whether to handle multimodals as main task - audio/image/video <code>image</code> Optional[str] None Absolute file path or URL to the image file (either for prompt context or main task) <code>file</code> Optional[str] None Absolute file path or URL to the file <code>audio</code> Optional[str] None Absolute file path or URL to the audio file <code>should_test_run</code> bool False Whether to turn on auto-feedback learning <code>human</code> bool False Whether to ask human input during the task execution <code>execution_type</code> TaskExecutionType TaskExecutionType.SYNC Sync or async execution <code>allow_delegation</code> bool False Whether to allow the agent to delegate the task to another agent <code>callback</code> Optional[Callable] None Callback function to be executed after LLM calling <code>callback_kwargs</code> Optional[Dict[str, Any]] dict() Args for the callback function (if any) <code>should_evaluate</code> bool False Whether to evaluate the task output using eval criteria <code>eval_criteria</code> Optional[List[str]] list() Evaluation criteria given by the human client <code>fsls</code> Optional[List[str]] None Examples of competitive and/or weak responses <code>processed_agents</code> Set[str] set() Stores keys of agents that executed the task <code>output</code> Optional[TaskOutput] None Stores <code>TaskOutput</code> object after the execution"},{"location":"core/task/reference.html#class-methods","title":"Class Methods","text":"Method Params Returns Description <code>execute</code> <p>type: TaskExecutionType = Noneagent: Optional[\"vhq.Agent\"] = Nonecontext: Optional[Any] = None</p> InstanceOf[<code>TaskOutput</code>] or None (error) A main method to handle task execution. Auto-build an agent when the agent is not given."},{"location":"core/task/reference.html#properties","title":"Properties","text":"Property Data_Type Description <code>key</code> str Returns task key based on its description and output format. <code>summary</code> str Returns a summary of the task based on its id, description and tools."},{"location":"core/task/reference.html#class-responsefield","title":"Class <code>ResponseField</code>","text":""},{"location":"core/task/reference.html#variables_1","title":"Variables","text":"Variable Data Type Default Description <code>title</code> str None Stores a field title. <code>data_type</code> Type None Stores data type of the response. <code>items</code> Type None Stores data type of items in the list. <code>None</code> when <code>data_type</code> is not list. <code>properties</code> List[<code>ResponseField</code>] None Stores properties in a list of <code>ResponseFormat</code> objects when the <code>data_type</code> is dict. <code>nullable</code> bool False If the field is nullable. <code>config</code> Dict[str, Any] None Stores other configs passed to response schema."},{"location":"core/task/reference.html#class-taskoutput","title":"Class <code>TaskOutput</code>","text":""},{"location":"core/task/reference.html#variables_2","title":"Variables","text":"Variable Data Type Default Description <code>task_id</code> UUID uuid.uuid4() Stores task <code>id</code> as an identifier. <code>raw</code> str None Stores response in plane text format. <code>None</code> or <code>\"\"</code> when the model returned errors. <code>json_dict</code> Dict[str, Any] None Stores response in JSON serializable dictionary. <code>pydantic</code> Type[<code>BaseModel</code>] None Populates the given response schema in pydantic class <code>tool_output</code> Optional[Any] None Stores results from the tools of the task or agents when <code>tool_res_as_final</code> == True <code>callback_output</code> Optional[Any] None Stores results from callback functions if any. <code>annotations</code> Optional[Dict[str, Any]] None Stores annotations given by the model. <code>evaluation</code> Optional[InstanceOf[<code>Evaluation</code>]] None Stores overall evaluations and usage of the task output. <code>usage</code> Optional[InstanceOf[<code>UsageMetrics]</code>] None Usage related values."},{"location":"core/task/reference.html#class-methods_1","title":"Class Methods","text":"Method Params Returns Description <code>evaluate</code> task: InstanceOf[<code>Task</code>] InstanceOf[<code>Evaluation</code>] Evaluates task output based on the criteria"},{"location":"core/task/reference.html#properties_1","title":"Properties","text":"Property Data_Type Description <code>final</code> Any Returns the final output of the task <code>aggregate_score</code> float Calucurates weighted average eval scores of the task output. <code>json_string</code> str Returns <code>json_dict</code> in string format."},{"location":"core/task/reference.html#class-evaluation","title":"Class <code>Evaluation</code>","text":""},{"location":"core/task/reference.html#variables_3","title":"Variables","text":"Variable Data Type Default Description <code>items</code> List[InstanceOf[EvaluationItem]] list() Stores evaluation items. <code>eval_by</code> Optional[InstanceOf[Agent]] None Stores an agent assigned to evaluate the output."},{"location":"core/task/reference.html#properties_2","title":"Properties","text":"Property Data_Type Description <code>aggregate_score</code> float Calucurates weighted average eval scores of the task output. <code>suggestion_summary</code> str Returns summary of the suggestions."},{"location":"core/task/reference.html#sub-class-evaluationitem","title":"Sub-class <code>EvaluationItem</code>","text":""},{"location":"core/task/reference.html#variables_4","title":"Variables","text":"Variable Data Type Default Description <code>criteria</code> str None Stores evaluation criteria given by the client. <code>suggestion</code> str None Stores suggestion on improvement from the evaluator agent. <code>score</code> float None Stores the score on a 0 to 1 scale. <code>weight</code> int None Stores the weight (importance of the criteria) at any scale."},{"location":"core/task/response-field.html","title":"Response Field","text":"<p><code>class</code> versionhq.task.model.ResponseField <p>A Pydantic class to store response formats to generate a structured response in JSON.</p> <p>Quick Start</p> <p>Define a response format with field titles and data types.</p> <pre><code>import versionhq as vhq\n\nresponse_field = vhq.ResponseField(\n  title=\"summary\",\n  data_type=str,\n  nullable=False, # default = False. Explicitly mentioned.\n)\n\n## Agent output:\n#  \"summary\": &lt;AGENT_RESPONSE_IN_STRING&gt;\n</code></pre>"},{"location":"core/task/response-field.html#object","title":"Object","text":"<p><code>[var]</code><code>properties: List[InstanceOf[ResponseField]] = None</code></p> <p>To format an object, add <code>ResponseField</code> objects to the <code>properties</code> fields.</p> <p>Missing properties for dict will trigger an error.</p> <pre><code>import versionhq as vhq\n\nresponse_field = vhq.ResponseField(\n  title=\"dict-summary\",\n  data_type=dict,\n  nullable=False,\n  properties=[\n    vhq.ResponseField(title=\"summary-1\", data_type=str),\n    vhq.ResponseField(title=\"summary-2\", data_type=int),\n  ]\n)\n\n## Agent output:\n#  dict-summary: {\n#   \"summary-1\": &lt;AGENT_RESPONSE_1_IN_STRING&gt;,\n#   \"summary-2\": &lt;AGENT_RESPONSE_2_IN_INTEGER&gt;,\n# }\n</code></pre>"},{"location":"core/task/response-field.html#list","title":"List","text":"<p><code>[var]</code><code>items: Optional[Type] = None</code></p> <p>To format a list, add data types of the list items to the <code>items</code> field.</p> <p>Missing items for list will trigger an error.</p> <pre><code>import versionhq as vhq\n\nresponse_field = vhq.ResponseField(\n  title=\"list-summary\",\n  data_type=list,\n  nullable=False,\n  items=str\n)\n\n## Agent output:\n#  list-summary: [\n#     &lt;AGENT_RESPONSE_1_IN_STRING&gt;,\n#     &lt;AGENT_RESPONSE_2_IN_STRING&gt;,\n#     ...\n# ]\n</code></pre>"},{"location":"core/task/response-field.html#nesting","title":"Nesting","text":"<p>Agents can handle one layer of nested items usign <code>properties</code> and <code>items</code> fields.</p> <p>We highly recommend to use <code>gemini-x</code> or <code>gpt-x</code> to get stable results.</p>"},{"location":"core/task/response-field.html#object-in-list","title":"Object in List","text":"<pre><code>import versionhq as vhq\n\nlist_with_objects = vhq.ResponseField(\n  title=\"parent\",\n  data_type=list,\n  items=dict,\n  properties=[\n    vhq.ResponseField(title=\"nest-1\", data_type=str),\n    vhq.ResponseField(title=\"nest-2\", data_type=float),\n  ]\n)\n\n# Agent output\n# parent: [\n#   { \"nest-1\": &lt;AGENT_RESOPONSE_IN_STRING&gt;},\n#   { \"nest-2\": &lt;AGENT_RESOPONSE_IN_NUMBER&gt;},\n# ]\n</code></pre>"},{"location":"core/task/response-field.html#list-in-list","title":"List in List","text":"<pre><code>import versionhq as vhq\n\nlist_with_list = vhq.ResponseField(\n  title=\"parent\",\n  data_type=list,\n  items=list\n)\n\n# Agent output\n# parent: [\n#   [&lt;AGENT_RESOPONSE_IN_STRING&gt;, ...],\n#   [&lt;AGENT_RESOPONSE_IN_STRING&gt;, ...]\n#   ...\n# ]\n</code></pre>"},{"location":"core/task/response-field.html#list-in-object","title":"List in Object","text":"<pre><code>import versionhq as vhq\n\ndict_with_list = vhq.ResponseField(\n  title=\"parent\",\n  data_type=dict,\n  properties=[\n    vhq.ResponseField(title=\"nest-1\", data_type=list, items=str),\n    vhq.ResponseField(title=\"nest-2\", data_type=list, items=int),\n  ]\n)\n\n# Agent output\n# parent: {\n#   nest-1: [&lt;AGENT_RESOPONSE_IN_STRING&gt;, ...],\n#   nest-2: [&lt;AGENT_RESOPONSE_IN_INTEGER&gt;, ...]\n# }\n</code></pre>"},{"location":"core/task/response-field.html#object-in-object","title":"Object in Object","text":"<pre><code>import versionhq as vhq\n\ndict_with_dict = vhq.ResponseField(\n  title=\"parent\",\n  data_type=dict,\n  properties=[\n    vhq.ResponseField(title=\"nest-1\", data_type=dict, properties=[\n      vhq.ResponseField(title=\"nest-1-1\", data_type=str)\n    ]),\n    vhq.ResponseField(title=\"nest-2\", data_type=list, items=int),\n  ]\n)\n\n# Agent output\n# parent: {\n#   nest-1: { nest-1-1: &lt;AGENT_RESOPONSE_IN_STRING&gt;, },\n#   nest-2: [&lt;AGENT_RESOPONSE_IN_INTEGER&gt;, ...]\n# }\n</code></pre>"},{"location":"core/task/response-field.html#config","title":"Config","text":"<p><code>[var]</code><code>config: Optional[Dict[str, Any]] = {}</code></p> <p>You can add other configs you want to pass to the LLM.</p> <pre><code>import versionhq as vhq\n\nresponse_field = vhq.ResponseField(\n  title=\"summary-with-config\",\n  data_type=str,\n  nullable=False,\n  config=dict(required=False, )\n)\n\n# Agent output:\n# summary-with-config: &lt;AGENT_RESPONSE_IN_STRING&gt;\n</code></pre> <p>Ref. List of variables</p>"},{"location":"core/task/task-execution.html","title":"Executing Task","text":""},{"location":"core/task/task-execution.html#prompt-engineering","title":"Prompt Engineering","text":"<p>Prompt messages are generated automatically based on the task <code>description</code>, response format, context, agent <code>role</code>, and <code>goal</code>.</p> <p>Context</p> <p>The following snippet demonstrates adding <code>context</code> from tools and sub tasks as well as <code>multi-modal content</code> to the prompt.</p> <pre><code>import versionhq as vhq\n\nagent = vhq.Agent(llm=\"gemini-2.0\", role=\"Content Interpretator\", tools=[rag_tool])\ntask = vhq.Task(description=\"dummy task\", image=\"image file path\")\nsub_task = vhq.Task(description=\"sub task\")\nrag_tool = vhq.RagTool(url=\"https://github.com/chroma-core/chroma/issues/3233\", query=\"What is the next action plan?\")\n\n\n# Explicitly mentioned for explanation purpose. By default, `task.execute()` will trigger this formula.\nfrom versionhq._prompt.model import Prompt\n_, _, messages = Prompt(task=main_task, agent=agent, context=[\"context 1\", \"context 2\", sub_task]).format_core()\n\nassert messages[0][\"role\"] == \"user\"\nassert isinstance(messages[0][\"content\"], list) # adding context and image, file, audio data to the prompt\nassert messages[1][\"role\"] == \"developer\" # adding developer prompt\nassert messages[1][\"content\"] == agent.backstory\n</code></pre> <p>Context can consist of <code>Task</code> objects, <code>TaskOutput</code> objects, plain text <code>strings</code>, or <code>lists</code> containing any of these.</p> <p>In this scenario, <code>sub_task_2</code> executes before the main task. Its string output is then incorporated into the main task's context prompt on top of other context before the main task is executed.</p> <p>Auto Feedback Learning</p> <p>To automatically improve prompts, trigger <code>test_run</code> of the task.</p> <pre><code>import versionhq as vhq\n\ntask = vhq.Task(description=\"Create a short story.\", should_test_run=True, human=True)\nres = task.execute()\n\nassert isinstance(res, vhq.TaskOutput)\n</code></pre>"},{"location":"core/task/task-execution.html#delegation","title":"Delegation","text":"<p><code>[var]</code><code>allow_delegation: bool = False</code></p> <p>You can assign another agent to complete the task:</p> <pre><code>import versionhq as vhq\n\ntask = vhq.Task(\n    description=\"return the output following the given prompt.\",\n    allow_delegation=True\n)\ntask.execute()\n\nassert task.output is not None\nassert task.processed_agents is not None # auto assigned\nassert task._delegations ==1\n</code></pre>"},{"location":"core/task/task-execution.html#sync-async-execution","title":"Sync - Async Execution","text":"<p><code>[var]</code><code>type: bool = False</code></p> <p>You can specify whether the task will be executed asynchronously.</p> <pre><code>import versionhq as vhq\n\ntask = vhq.Task(\n    description=\"Return a word: 'test'\",\n    type=vhq.TaskExecutionType.ASYNC # default: vhq.TaskExecutionType.SYNC\n)\n\nfrom unittest.mock import patch\nwith patch.object(vhq.Agent, \"execute_task\", return_value=(\"user prompt\", \"dev prompt\", \"test\")) as execute:\n    res = task.execute()\n    assert res.raw == \"test\"\n    execute.assert_called_once_with(task=task, context=None, task_tools=list())\n</code></pre>"},{"location":"core/task/task-execution.html#tools","title":"Tools","text":"<p><code>[var]</code><code>tools: Optional[List[ToolSet | Tool | Any]] = None</code></p> <p><code>[var]</code><code>tool_res_as_final: bool = False</code></p> <p>Tasks can directly store tools explicitly called by the agent.</p> <p>If the results from the tool should be the final results, set <code>tool_res_as_final</code> True.</p> <p>This will allow the agent to store the tool results in the <code>tool_output</code> field of <code>TaskOutput</code> object.</p> <pre><code>import versionhq as vhq\nfrom typing import Callable\n\ndef random_func(message: str) -&gt; str:\n    return message + \"_demo\"\n\ntool = vhq.Tool(name=\"tool\", func=random_func)\ntool_set = vhq.ToolSet(tool=tool, kwargs=dict(message=\"empty func\"))\ntask = vhq.Task(\n    description=\"execute the given tools\",\n    tools=[tool_set,], # stores tools\n    tool_res_as_final=True, # stores tool results in TaskOutput object\n)\n\nres = task.execute()\nassert res.tool_output == \"empty func_demo\"\n</code></pre> <p>Ref 1. Tool class / RAGTool class</p> <p>Ref 2. TaskOutput class</p> <p>Using agents' tools</p> <p><code>[var]</code><code>can_use_agent_tools: bool = True</code></p> <p>Tasks can explicitly stop/start using agent tools on top of the tools stored in the task object.</p> <pre><code>import versionhq as vhq\n\nsimple_tool = vhq.Tool(name=\"simple tool\", func=lambda x: \"simple func\")\nagent = vhq.Agent(role=\"demo\", goal=\"execute tools\", tools=[simple_tool,])\ntask = vhq.Task(\n    description=\"execute tools\",\n    can_use_agent_tools=True, # Flagged\n    tool_res_as_final=True\n)\nres = task.execute(agent=agent)\nassert res.tool_output == \"simple func\"\n</code></pre>"},{"location":"core/task/task-execution.html#image-audio-file-content","title":"Image, Audio, File Content","text":"<p>Refer the content by adding an absolute file path to the content file or URL to the task object.</p> <pre><code>import versionhq as vhq\nfrom pathlib import Path\n\ncurrent_dir = Path(__file__).parent.parent\nfile_path = current_dir / \"_sample/screenshot.png\"\naudio_path = current_dir / \"_sample/sample.mp3\"\n\ntask = vhq.Task(description=\"Summarize the given content\", image=str(file_path), audio=str(audio_path))\nres = task.execute(agent=vhq.Agent(llm=\"gemini-2.0\", role=\"Content Interpretator\"))\n\nassert res.raw is not None\n</code></pre> <ul> <li>Audio files are only applicable to <code>gemini</code> models.</li> </ul>"},{"location":"core/task/task-execution.html#callbacks","title":"Callbacks","text":"<p><code>[var]</code><code>callback: Optional[Callable] = None</code></p> <p><code>[var]</code><code>callback_kwargs: Optional[Dict[str, Any]] = dict()</code></p> <p>After executing the task, you can run a <code>callback</code> function with <code>callback_kwargs</code> and task output as parameters.</p> <p>Callback results will be stored in <code>callback_output</code> filed of the <code>TaskOutput</code> object.</p> <pre><code>import versionhq as vhq\n\ndef callback_func(condition: str, test1: str, **kwargs):\n    return f\"Result: {test1}, condition added: {condition}\"\n\ntask = vhq.Task(\n    description=\"return the output following the given prompt.\",\n    callback=callback_func,\n    callback_kwargs=dict(condition=\"demo for pytest\")\n)\nres = task.execute()\n\nassert res and isinstance(res, vhq.TaskOutput)\nassert res.task_id is task.id\nassert \"demo for pytest\" in res.callback_output\n</code></pre>"},{"location":"core/task/task-output.html","title":"Task Output","text":"<p><code>class</code> versionhq.task.model.TaskOutput <p>A Pydantic class to store and manage results of <code>Task</code>.</p> <p>The following snippet demonstrates the  <code>TaskOutput</code> object when the task is all-in with Pydantic response format, callbacks, tools, and evaluation.</p> <pre><code>import versionhq as vhq\nfrom pydantic import BaseModel\n\nclass CustomOutput(BaseModel):\n    test1: str\n    test2: list[str]\n\ndef dummy_tool():\n    return \"dummy\"\n\ndef summarize_response(message: str, **kwargs) -&gt; str | None:\n    test1 = kwargs[\"test1\"] if kwargs and \"test1\" in kwargs else None\n    test2 = kwargs[\"test2\"] if kwargs and \"test2\" in kwargs else None\n    if test1 and test2:\n        return f\"\"\"{message}: {test1}, {\", \".join(str(test2))}\"\"\"\n    else:\n        return None\n\ntask = vhq.Task(\n    description=\"Research a topic to teach a kid aged 6 about math.\",\n    response_schema=CustomOutput,\n    tools=[dummy_tool],\n    callback=summarize_response,\n    callback_kwargs=dict(message=\"Hi! Here is the result: \"),\n    should_evaluate=True, # triggers evaluation\n    eval_criteria=[\"Uniquness\", \"Fit to audience\",],\n\n)\nres = task.execute()\n\nassert res.task_id == task.id\nassert res.raw\nassert res.json_dict\nassert res.pydantic.test1 and res.pydantic.test2\nif res.callback_output:\n    assert \"Hi! Here is the result: \" in res.callback_output\nassert res.tool_output is None\nassert res.evaluation and isinstance(res.evaluation, vhq.Evaluation)\n</code></pre> <p>The <code>TaskOutput</code> object has a <code>final</code> field that contains the task's definitive result.</p> <p>Result priority is: <code>callback</code> &gt; <code>tool</code> (when tool_res_as_final is true) &gt; <code>pydantic</code> &gt; <code>json_dict</code> &gt; <code>raw</code> output.</p> <pre><code>assert res.final == res.callback_output\n</code></pre> <p>Ref. List of variables and class methods</p>","tags":["Task Graph"]},{"location":"core/task/task-strc-response.html","title":"Structured Response","text":"<p>By default, agents will generate plane text and JSON outputs, and store them in the <code>TaskOutput</code> object.</p> <ul> <li>Ref. <code>TaskOutput</code> class</li> </ul> <p>But you can choose to generate Pydantic class or specifig JSON object as response.</p> <p><code>[var]</code><code>response_schema: Optional[Type[BaseModel] | List[ResponseField]] = None</code></p>"},{"location":"core/task/task-strc-response.html#1-pydantic","title":"1. Pydantic","text":"<p>Add a <code>custom Pydantic class</code> to the <code>response_schema</code> field to generate a structured response.</p> <p>The custom class can accept one layer of a nested child as you can see in the following code snippet:</p> <pre><code>import versionhq as vhq\nfrom pydantic import BaseModel\nfrom typing import Any\n\n\n# 1. Define Pydantic class with a description (optional), annotations and field names.\nclass Demo(BaseModel):\n    \"\"\"\n    A demo pydantic class to validate the outcome with various nested data types.\n    \"\"\"\n    demo_1: int\n    demo_2: float\n    demo_3: str\n    demo_4: bool\n    demo_5: list[str]\n    demo_6: dict[str, Any]\n    demo_nest_1: list[dict[str, Any]] # 1 layer of nested child is ok.\n    demo_nest_2: list[list[str]]\n    demo_nest_3: dict[str, list[str]]\n    demo_nest_4: dict[str, dict[str, Any]]\n    # error_1: list[list[dict[str, list[str]]]] # &lt;- Trigger 400 error due to 2+ layers of nested child.\n    # error_2: InstanceOf[AnotherPydanticClass] # &lt;- Trigger 400 error due to non-typing annotation.\n    # error_3: list[InstanceOf[AnotherPydanticClass]] # &lt;- Trigger 400 error due to non-typing annotation as a nested child.\n\n# 2. Define a task\ntask = vhq.Task(\n    description=\"generate random output that strictly follows the given format\",\n    response_schema=Demo,\n)\n\n# 3. Execute\nres = task.execute()\n\nassert isinstance(res, vhq.TaskOutput)\nassert res.raw and res.json\nassert isinstance(res.raw, str) and isinstance(res.json_dict, dict)\nassert [\n    getattr(res.pydantic, k) and v.annotation == Demo.model_fields[k].annotation\n    for k, v in res.pydantic.model_fields.items()\n]\n</code></pre>"},{"location":"core/task/task-strc-response.html#2-json","title":"2. JSON","text":"<p>Similar to Pydantic, JSON output structure can be defined by using a list of <code>ResponseField</code> objects.</p> <p>The following code snippet demonstrates how to use <code>ResponseField</code> to generate output with a maximum of one level of nesting.</p> <p>Custom JSON outputs can accept one layer of a nested child.</p> <p>[NOTES]</p> <ul> <li> <p><code>demo_response_fields</code> in the following case is identical to the previous Demo class, except that titles are specified for nested fields.</p> </li> <li> <p>Agents generate JSON output by default, whether or not <code>response_schema</code> is given.</p> </li> <li> <p>However, <code>response_schema</code> is required to specify the key value sets.</p> </li> </ul> <pre><code>import versionhq as vhq\n\n# 1. Define a list of ResponseField objects.\ndemo_response_fields = [\n    vhq.ResponseField(title=\"demo_1\", data_type=int),\n    vhq.ResponseField(title=\"demo_2\", data_type=float),\n    vhq.ResponseField(title=\"demo_3\", data_type=str),\n    vhq.ResponseField(title=\"demo_4\", data_type=bool),\n    vhq.ResponseField(title=\"demo_5\", data_type=list, items=str),\n    vhq.ResponseField(\n        title=\"demo_6\",\n        data_type=dict,\n        properties=[vhq.ResponseField(title=\"demo-item\", data_type=str)]\n    ),\n    # nesting\n    vhq.ResponseField(\n        title=\"demo_nest_1\",\n        data_type=list,\n        items=dict,\n        properties=([\n            vhq.ResponseField(\n                title=\"nest1\",\n                data_type=dict,\n                properties=[vhq.ResponseField(title=\"nest11\", data_type=str)]\n            )\n        ])\n    ),\n    vhq.ResponseField(title=\"demo_nest_2\", data_type=list, items=list),\n    vhq.ResponseField(title=\"demo_nest_3\", data_type=dict, properties=[\n        vhq.ResponseField(title=\"nest1\", data_type=list, items=str)\n    ]),\n    vhq.ResponseField(title=\"demo_nest_4\", data_type=dict, properties=[\n        vhq.ResponseField(\n            title=\"nest1\",\n            data_type=dict,\n            properties=[vhq.ResponseField(title=\"nest12\", data_type=str)]\n        )\n    ])\n]\n\n\n# 2. Define a task\ntask = vhq.Task(\n    description=\"Output random values strictly following the data type defined in the given response format.\",\n    response_schema=demo_response_fields\n)\n\n\n# 3. Execute\nres = task.execute()\n\nassert isinstance(res, vhq.TaskOutput)\nassert [v and type(v) == task.response_schema[i].data_type for i, (k, v) in enumerate(res.json_dict.items())]\n</code></pre> <ul> <li>Ref. <code>ResponseField</code> class</li> </ul> <p>Structuring reponse format</p> <ul> <li> <p>Higlhy recommends assigning agents optimized for <code>gemini-x</code> or <code>gpt-x</code> to produce structured outputs with nested items.</p> </li> <li> <p>To generate response with more than 2 layers of nested items, seperate them into multipe tasks or utilize nodes.</p> </li> </ul> <p>The following case demonstrates to returning a <code>Main</code> class that contains a nested <code>Sub</code> class.</p> <p>[NOTES]</p> <ul> <li> <p>Using <code>callback</code> functions to format the final response. (You can try other functions suitable for your use case.)</p> </li> <li> <p>Passing parameter: <code>sub</code> to the callback function via the <code>callback_kwargs</code> variable.</p> </li> <li> <p>By default, the outputs of <code>main_task</code> are automatically passed to the callback function; you do NOT need to explicitly define them.</p> </li> <li> <p>Callback results will be stored in the <code>callback_output</code> field of the <code>TaskOutput</code> class.</p> </li> </ul> <pre><code>import versionhq as vhq\nfrom pydantic import BaseModel\nfrom typing import Any\n\n# 1. Defines a sub task\nclass Sub(BaseModel):\n    sub1: list[dict[str, Any]]\n    sub2: dict[str, Any]\n\nsub_task = vhq.Task(\n    description=\"generate random values that strictly follows the given format.\",\n    response_schema=Sub\n)\nsub_res = sub_task.execute()\n\n# 2. Defines a main task with callbacks\nclass Main(BaseModel):\n    main1: list[Any] # &lt;= assume expecting to store Sub object.\n    main2: dict[str, Any]\n\n\ndef format_response(sub, **kwargs) -&gt; Main:\n    main1 = kwargs[\"main1\"] if kwargs and \"main1\" in kwargs else None\n    if main1:\n        main1.append(sub)\n    main2 = kwargs[\"main2\"] if kwargs and \"main2\" in kwargs else None\n    main = Main(main1=main1, main2=str(main2))\n    return main\n\n# 3. Executes\nmain_task = vhq.Task(\n    description=\"generate random values that strictly follows the given format.\",\n    response_schema=Main,\n    callback=format_response,\n    callback_kwargs=dict(sub=sub_res.json_dict),\n)\nres = main_task.execute(context=sub_res.raw) # [Optional] Adding sub_task's response as context.\n\nassert res.callback_output.main1 is not None\nassert res.callback_output.main2 is not None\n</code></pre> <p>To automate these manual setups, refer to AgentNetwork class.</p>"},{"location":"core/task-graph/index.html","title":"TaskGraph","text":"<p><code>class</code> versionhq.graph.model.TaskGraph <p>A <code>TaskGraph</code> represents tasks as <code>nodes</code> and their execution dependencies as <code>edges</code>, automating rule-based execution.</p> <p><code>Agent Networks</code> can handle <code>TaskGraph</code> objects by optimizing their formations.</p> <p>The following example demonstrates a simple concept of a <code>supervising</code> agent network handling a task graph with three tasks and one critical edge.</p> <p></p>","tags":["Task Graph"]},{"location":"core/task-graph/index.html#quick-start","title":"Quick Start","text":"<p><code>TaskGraph</code> needs at least two <code>nodes</code> and one <code>edges</code> to connect the nodes to function.</p> <p>You can define nodes and edges mannually by creating nodes from tasks, and defining edges.</p>","tags":["Task Graph"]},{"location":"core/task-graph/index.html#generating","title":"Generating","text":"<pre><code>import versionhq as vhq\n\ntask_graph = vhq.TaskGraph(directed=False, should_reform=True)\n\ntask_a = vhq.Task(description=\"Research Topic\")\ntask_b = vhq.Task(description=\"Outline Post\")\ntask_c = vhq.Task(description=\"Write First Draft\")\n\nnode_a = task_graph.add_task(task=task_a)\nnode_b = task_graph.add_task(task=task_b)\nnode_c = task_graph.add_task(task=task_c)\n\ntask_graph.add_dependency(\n    node_a.identifier, node_b.identifier,\n    dependency_type=vhq.DependencyType.FINISH_TO_START, weight=5, description=\"B depends on A\"\n)\ntask_graph.add_dependency(\n    node_a.identifier, node_c.identifier,\n    dependency_type=vhq.DependencyType.FINISH_TO_FINISH, lag=1, required=False, weight=3\n)\n\ncritical_path, duration, paths = task_graph.find_critical_path()\n\nimport uuid\nassert isinstance(task_graph, vhq.TaskGraph)\nassert [type(k) == uuid.uuid4 and isinstance(v, vhq.Node) and isinstance(v.task, vhq.Task) for k, v in task_graph.nodes.items()]\nassert [type(k) == uuid.uuid4 and isinstance(v, vhq.Edge) for k, v in task_graph.edges.items()]\nassert critical_path  and duration  and paths\n</code></pre>","tags":["Task Graph"]},{"location":"core/task-graph/index.html#activating","title":"Activating","text":"<p>Calling <code>.activate()</code> begins execution of the graph's nodes, respecting dependencies [<code>dependency-met</code>] and prioritizing the critical path.</p> <p>[NOTES]</p> <ul> <li> <p>If all nodes are already complete, outputs are returned without further execution.</p> </li> <li> <p>If no critical path is found, execution begins with any dependency-met start nodes.</p> </li> </ul> <pre><code>import versionhq as vhq\n\n# Inherting the `task_graph` object in the previous code snippet,\n\nlast_task_output, outputs = task_graph.activate()\n\nassert isinstance(last_task_output, vhq.TaskOutput)\nassert [k in task_graph.nodes.keys() and v and isinstance(v, vhq.TaskOutput) for k, v in outputs.items()]\n</code></pre>","tags":["Task Graph"]}]}